\documentclass[12pt]{amsart}
\usepackage{amstext,amsfonts,amssymb,amscd,amsbsy,amsmath,verbatim}
\usepackage{ifthen}
\usepackage{color,tikz}

\usepackage{amsthm}
\usepackage{latexsym}
\usepackage[all]{xy}
\usepackage{enumerate}
\usepackage{url}

\newtheorem{lemma}{Lemma}[section]
\newtheorem{theorem}[lemma]{Theorem}
\newtheorem{propo}[lemma]{Proposition}

\newtheorem{prop}[lemma]{Proposition}
\newtheorem{cor}[lemma]{Corollary}
\newtheorem{conj}[lemma]{Conjecture}
\newtheorem{claim}[lemma]{Claim}
\newtheorem{claim*}{Claim}
\newtheorem{thm}[lemma]{Theorem}
\newtheorem{defn}[lemma]{Definition}
\newtheorem{example}[lemma]{Example}
\newtheorem{definition}[lemma]{Definition}


\theoremstyle{remark}
\newtheorem{remark}[lemma]{Remark}

\usepackage{geometry,enumerate}
\geometry{a4paper, top=3.5cm, bottom=3cm, left=3cm, right=3cm}

\parindent = 6pt
\parskip = 4pt

% Commands
\newcommand{\isom}{\cong}
\newcommand{\m}{\mathfrak m}
\newcommand{\lideal}{\langle}
\newcommand{\rideal}{\rangle}
\newcommand{\initial}{\operatorname{in}}
\newcommand{\Hilb}{\operatorname{Hilb}}
\newcommand{\Spec}{\operatorname{Spec}}
\newcommand{\im}{\operatorname{im}}
\newcommand{\NS}{\operatorname{NS}}
\newcommand{\Frac}{\operatorname{Frac}}
\newcommand{\ch}{\operatorname{char}}
\newcommand{\Proj}{\operatorname{Proj}}
\newcommand{\id}{\operatorname{id}}
\newcommand{\Div}{\operatorname{Div}}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\Tr}{\operatorname{Tr}}
\newcommand{\Supp}{\operatorname{Supp}}
\newcommand{\Gal}{\operatorname{Gal}}
\newcommand{\Pic}{\operatorname{Pic}}
\newcommand{\QQbar}{{\overline{\mathbb Q}}}
\newcommand{\Br}{\operatorname{Br}}
\newcommand{\Bl}{\operatorname{Bl}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\grad}{\nabla}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\Cox}{\operatorname{Cox}}
\newcommand{\Tor}{\operatorname{Tor}}
\newcommand{\diam}{\operatorname{diam}}
\newcommand{\Hom}{\operatorname{Hom}} %done
\newcommand{\sheafHom}{\mathcal{H}om}
\newcommand{\Gr}{\operatorname{Gr}}
\newcommand{\Osh}{{\mathcal O}}
\newcommand{\kk}{\kappa}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\codim}{\operatorname{codim}}
\newcommand{\conv}{\operatorname{conv}}
\newcommand{\D}{{\mathcal D}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\EE}{\mathbb{E}}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\zz}{\mathbb{Z}}
\newcommand{\Sym}{\operatorname{Sym}} %done
\newcommand{\GL}{{GL}}
\newcommand{\Syz}{\operatorname{Syz}}
\newcommand{\defi}[1]{\textsf{#1}} % for defined terms


\newcommand{\Bmod}{\ensuremath{B_
\text{mod}}}
\newcommand{\Bint}{\ensuremath{B_\text{int}}}
\newcommand\commentr[1]{{\color{red} \sf [#1]}}
\newcommand\commentb[1]{{\color{blue} \sf [#1]}}
\newcommand\commentm[1]{{\color{magenta} \sf [#1]}}
\newcommand{\ddr}[1]{{\color{blue} \sf $\clubsuit\clubsuit\clubsuit$ Daniel: [#1]}} 
\newcommand{\mv}[1]{{\color{red} \sf $\clubsuit\clubsuit\clubsuit$ Mauricio: [#1]}}


\title{Graph learning and the Wasserstein metric}
\author{Daniel De Roux}
\author{Mauricio Velasco}


\begin{document}
\maketitle


\section{Preliminaries}


\subsection{ Preliminaries on graphs}

By a graph $G$ we mean a finite loopless undirected graph. We say that $G$ is weighted if it is endowed with a function $w: E(G)\rightarrow \RR$ which assigns to every edge a real number in $[0,1]$. If $G$ has $n$ vertices then it is completely specified by its adjacency matrix $A\in \{0,1\}^{n\times n}$ defined by $A_{ij}=1$ if and only if vertices $i,j$ are connected. If $G$ is weighted then we use the term adjacency matrix of $G$ to denote the matrix with entries $A_{i,j}=w(i,j)$. 

If $A$ is a matrix then we use $\|\bullet\|$, $\|\bullet\|_1$ to denote its operator norm and $\ell^1$-norm respectively.

\section{A description of the problem}

By a random graph $B$ we mean a random variable $B$ taking values on the set of adjacency matrices of graphs (i.e. symmeric matrices in $\{0,1\}^{n\times n}$ with zero diagonal). By a random weighted graph we mean a random variable taking values in the adjacency matrices of weighted graphs (i.e. symmetric matrices with $0$ diagonal all of whose off-diagonal entries lie in $[0,1]$). 

\begin{definition} Let $B$ be a random weighted graph and let $A$ be a (weighted) adjacency matrix. Define the risk of choosing $A\in \{0,1\}^n$ as a deterministic summary of $B$ as
\[R(A):=\EE[\|A-B\|_1].\]
We say that $A^*$ is an optimal summary of a random graph $B$ in the set $S$ if it is a minimizer of the optimization problem $\min_{A\in S} R(A)$.
\end{definition}


\section{Recovering cluster structures in the stochastic block model}

Suppose $B$ is a random (undirected loopless) graph on $n$ vertices generated by the stochastic block model. This means that we fix a set partition $C_1,\dots, C_l$ of $[n]$ into sets we call clusters and real numbers $0\leq p_i,\bar{p}\leq 1$ for $i=1,\dots, l$. The edges of $B$ are independent random variables and an edge joins vertices $i,j$ with probability $p_t$ if $\{i,j\}\subseteq C_t$ for some cluster $C_t$ and with probability $\bar{p}$ if $\{i,j\}$ is not contained in any $C_t$. We let $O\subseteq [n]\times [n]$ be the set of pairs of vertices which are not simultaneously contained in any cluster. 

For an integer $N$ let $B_1,\dots, B_N$ be an independent sample of $N$ graphs with the distribution of $B$. Let $A^*$ be the $n\times n$ matrix with entries in $\{0,1\}$ which captures the underlying cluster structure, namely $A^*_{ij}=1$ iff there is a cluster $C_t$ which contains both $ij$. In this section we study the probability, as a function of $\delta$ that the optimization problem $\min_A\Delta(A)$
\[\Delta(A)= \delta\|2A-11^t\|_{*}+\frac{1}{N}\sum_{k=1}^N\|A-B_k\|_1\] 
has the correct cluster structure $A^*$ as a minimizer. For vertices $i,j\in [n]$ define $n_1(ij)$ (resp. $n_0(ij)$) the random variables which count the number of times that a given pair is (resp is not) an edge of some $B_j$, $j=1,\dots N$. Note that the $n_q(ij)$ for $q=0,1$ are binomial random variables.  

\begin{lemma} The following statements hold:
\label{lem: subdiff}
\begin{enumerate}
\item The subdifferential of $\frac{1}{N}\sum_{k=1}^N\|A-B_k\|_1$ at $A^*$ is the set of symmetric matrices $C$ satisfying the inequalities
\[ \frac{n_0(ij)-n_1(ij)}{N}\leq C_{ij}\leq 1 \text{, if $\{i,j\}\subseteq C_t$ for some $t$ and}\]
\[-1\leq C_{ij} \leq \frac{n_0(ij)-n_1(ij)}{N} \text{ if $\{i,j\}$ does not belong to any cluster. } \]
\item The subdifferential of $\delta\|2A-11^t\|_{*}$ at $A^*$ is given by the set of symmetric matrices of the form $2\delta C$ where $C$ has spectral norm $\|C\|\leq 1$ and satisfies $\langle C, 2A-11^t\rangle = n$.

\end{enumerate}

\end{lemma}
\begin{proof} $(1)$ Since the subdifferential is additive it suffices to understand the subdifferential of the absolute value. If $i,j\in C_t$ then $A^*_{ij}=1$ and the entry $ij$ of the subdifferential of the sum at $A^*$ is $[-1,1]$ for each $B_i$ containing the edge and it is $1$ for each $B_i$ for which $(ij)$ is not an edge. If $i,j$ is not contained in any cluster then $A^*_{ij}=0$ and the entry $ij$ of the subdifferential of the sum at $A^*$ is $-1$ for each $B_i$ which contains the edge $ij$ and $[-1,1]$ for each $B_i$ which does not, proving the claim. $(2)$ It is easy to prove that the subdifferential of any norm $\|\bullet\|$ at a point $X$ is given by those $C$ for which the dual norm $\|C\|_*\leq 1$ and $\langle C,X\rangle =\|X\|$. Claim $(2)$ follows because $\|2A-11^t\|=Tr(2A-11^t)=n$ where the first equality holds since $2A-11^t$ is positive semidefinite.\mv{Esto es obvio con solo dos clusters (la matriz es $uu^t$ donde $u$ es el vector con $1$'s en un cluster y $-1$'s en el complemento pero hay que demostrarlo para tres o mas)}.
\end{proof}

\begin{lemma} If $\delta=0$ then 
\[\PP\{A^*\in\argmin\Delta \} =\prod_{ij\in O} \PP\{n_1(ij)\leq n_0(ij)\} \prod_{t=1}^k \left(\prod_{(ij)\in C_t}\PP\{n_0(ij)\leq n_1(ij)\}\right)\]
where $\PP\{n_0(ij)\leq n_1(ij)\}$ is given by the following formula 
\mv{Ejercicio para Daniel: Encontrar una f\'ormula, es la probabilidad de que haya mas caras que sellos en $N$ lanzamientos de una moneda trucada donde las probabilidades de la moneda dependen s\'olo de la arista $ij$}.
\end{lemma}
\begin{proof} The matrix $A^*$ is a minimizer of the above convex function if and only if its subdifferential at $A^*$ contains the matrix $0$. By part $(1)$ of the previous Lemma this occurs if and only if $\frac{n_0(ij)-n_1(ij)}{N}\geq 0$ for $(ij)$ in a cluster and 
$\frac{n_0(ij)-n_1(ij)}{N}\leq 0$ for $(ij)$ in $O$. Independence of the edges then implies the above formula.  
\end{proof}
\begin{remark} Could the set of minimizers be larger? If $A'$ has at least one entry $A_{ij}\in (0,1)$ the corresponding component in the subgradient is the constant $n_0(ij)-n_1(ij)$ and this equals zero with much smaller probability, precisely when both terms equal to $\frac{N}{2}$. In particular it is impossible if $N$ is odd and in this case $A^*$ is the only minimizer.
\end{remark}

Next we ask whether it is possible to increase the probability of correct recovery by allowing $\delta>0$. By Lemma~\ref{lem: subdiff} $A^*\in \argmin \Delta$ if and only if there exists $S$ in the subdifferential of $\delta\|2A-11^t\|_*$ at $A=A^*$ such that $-S$ belongs to the subdifferential of $\frac{1}{N}\sum_{k=1}^N\|A-B_k\|_1$ at $A^*$. 

The most immediate way to do this would be to find an $S$ with $S_{ij}\leq 0$ negative on edges $ij\in C_t$ and $S_{ij}\geq 0$ on edges of $O$. More precisely we would like to find a best such $C$ by solving the optimization problem:

\[
\min \left(\sum_{t}\sum_{(ij)\in C_t} C_{ij}\right)-\sum_{(ij)\in O} C_{ij} \text{ s.t. $\|B\|\leq 1$, $\langle C, 2A^*-11^t\rangle = n$} 
\]

However it is easy to see that we cannot do this improvement simultaneously in all components, because $n=\langle S, 2A-11^t\rangle = Tr(S)+\sum_{ij \in O^c} S_{ij} -\sum_{ij \in O} S_{ij}\leq n+u$ where $u$ is the objective function in the problem above. We conclude that $u$ must be nonnegative so we cannot improve simultaneously in all directions at once.

\mv{El pr\'oximo paso es entonces intentar hacer "tradeoffs", es decir quitar de una componente, dificultando la recuperaci\'on, para poner en otra haci\'endola m\'as f\'acil. En qu\'e direcci\'on hacer estos tradeoffs depender\'a entonces de poder entender las probabilidades relevantes con m\'as detalle -- es decir de las f\'ormulas de tarea. Algo que puede ayudar es de pronto usar la aproximaci√≥n de la binomial mediante la distribuci\'on normal.}





\end{document}


