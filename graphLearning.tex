\documentclass[12pt]{amsart}
\usepackage{amstext,amsfonts,amssymb,amscd,amsbsy,amsmath,verbatim}
\usepackage{ifthen}
\usepackage{color,tikz}

\usepackage{amsthm}
\usepackage{latexsym}
\usepackage[all]{xy}
\usepackage{enumerate}
\usepackage{url}

\newtheorem{lemma}{Lemma}[section]
\newtheorem{theorem}[lemma]{Theorem}
\newtheorem{propo}[lemma]{Proposition}

\newtheorem{prop}[lemma]{Proposition}
\newtheorem{cor}[lemma]{Corollary}
\newtheorem{conj}[lemma]{Conjecture}
\newtheorem{claim}[lemma]{Claim}
\newtheorem{claim*}{Claim}
\newtheorem{thm}[lemma]{Theorem}
\newtheorem{defn}[lemma]{Definition}
\newtheorem{example}[lemma]{Example}
\newtheorem{definition}[lemma]{Definition}


\theoremstyle{remark}
\newtheorem{remark}[lemma]{Remark}

\usepackage{geometry,enumerate}
\geometry{a4paper, top=3.5cm, bottom=3cm, left=3cm, right=3cm}

\parindent = 6pt
\parskip = 4pt

% Commands
\newcommand{\isom}{\cong}
\newcommand{\m}{\mathfrak m}
\newcommand{\lideal}{\langle}
\newcommand{\rideal}{\rangle}
\newcommand{\initial}{\operatorname{in}}
\newcommand{\Hilb}{\operatorname{Hilb}}
\newcommand{\Spec}{\operatorname{Spec}}
\newcommand{\im}{\operatorname{im}}
\newcommand{\NS}{\operatorname{NS}}
\newcommand{\Frac}{\operatorname{Frac}}
\newcommand{\ch}{\operatorname{char}}
\newcommand{\Proj}{\operatorname{Proj}}
\newcommand{\id}{\operatorname{id}}
\newcommand{\Div}{\operatorname{Div}}
\newcommand{\tr}{\operatorname{tr}}
\newcommand{\Tr}{\operatorname{Tr}}
\newcommand{\Supp}{\operatorname{Supp}}
\newcommand{\Gal}{\operatorname{Gal}}
\newcommand{\Pic}{\operatorname{Pic}}
\newcommand{\QQbar}{{\overline{\mathbb Q}}}
\newcommand{\Br}{\operatorname{Br}}
\newcommand{\Bl}{\operatorname{Bl}}
\newcommand{\cF}{\mathcal{F}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\grad}{\nabla}
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\Cox}{\operatorname{Cox}}
\newcommand{\Tor}{\operatorname{Tor}}
\newcommand{\diam}{\operatorname{diam}}
\newcommand{\Hom}{\operatorname{Hom}} %done
\newcommand{\sheafHom}{\mathcal{H}om}
\newcommand{\Gr}{\operatorname{Gr}}
\newcommand{\Osh}{{\mathcal O}}
\newcommand{\kk}{\kappa}
\newcommand{\rank}{\operatorname{rank}}
\newcommand{\codim}{\operatorname{codim}}
\newcommand{\conv}{\operatorname{conv}}
\newcommand{\D}{{\mathcal D}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\EE}{\mathbb{E}}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\zz}{\mathbb{Z}}
\newcommand{\Sym}{\operatorname{Sym}} %done
\newcommand{\GL}{{GL}}
\newcommand{\Syz}{\operatorname{Syz}}
\newcommand{\defi}[1]{\textsf{#1}} % for defined terms


\newcommand{\Bmod}{\ensuremath{B_
\text{mod}}}
\newcommand{\Bint}{\ensuremath{B_\text{int}}}
\newcommand\commentr[1]{{\color{red} \sf [#1]}}
\newcommand\commentb[1]{{\color{blue} \sf [#1]}}
\newcommand\commentm[1]{{\color{magenta} \sf [#1]}}
\newcommand{\ddr}[1]{{\color{blue} \sf $\clubsuit\clubsuit\clubsuit$ Daniel: [#1]}} 
\newcommand{\mv}[1]{{\color{red} \sf $\clubsuit\clubsuit\clubsuit$ Mauricio: [#1]}}


\title{Graph learning and the Wasserstein metric}
\author{Daniel De Roux}
\author{Mauricio Velasco}


\begin{document}
\maketitle


\section{Preliminaries}


\subsection{ Preliminaries on graphs}

By a graph $G$ we mean a finite loopless undirected graph. We say that $G$ is weighted if it is endowed with a function $w: E(G)\rightarrow \RR$ which assigns to every edge a real number in $[0,1]$. If $G$ has $n$ vertices then it is completely specified by its adjacency matrix $A\in \{0,1\}^{n\times n}$ defined by $A_{ij}=1$ if and only if vertices $i,j$ are connected. If $G$ is weighted then we use the term adjacency matrix of $G$ to denote the matrix with entries $A_{i,j}=w(i,j)$. 

If $A$ is a matrix then we use $\|\bullet\|$, $\|\bullet\|_1$ to denote its operator norm and $\ell^1$-norm respectively.

\section{A description of the problem}

By a random graph $B$ we mean a random variable $B$ taking values on the set of adjacency matrices of graphs (i.e. symmeric matrices in $\{0,1\}^{n\times n}$ with zero diagonal). By a random weighted graph we mean a random variable taking values in the adjacency matrices of weighted graphs (i.e. symmetric matrices with $0$ diagonal all of whose off-diagonal entries lie in $[0,1]$). 

\begin{definition} Let $B$ be a random weighted graph and let $A$ be a (weighted) adjacency matrix. Define the risk of choosing $A\in \{0,1\}^n$ as a deterministic summary of $B$ as
\[R(A):=\EE[\|A-B\|_1].\]
We say that $A^*$ is an optimal summary of a random graph $B$ in the set $S$ if it is a minimizer of the optimization problem $\min_{A\in S} R(A)$.
\end{definition}


\section{Recovering cluster structures in the stochastic block model}

Suppose $B$ is a random (undirected loopless) graph on $n$ vertices generated by the stochastic block model. This means that we fix a set partition $C_1,\dots, C_l$ of $[n]$ into sets we call clusters and real numbers $0\leq p_i,\bar{p}\leq 1$ for $i=1,\dots, l$. The edges of $B$ are independent random variables and an edge joins vertices $i,j$ with probability $p_t$ if $\{i,j\}\subseteq C_t$ for some cluster $C_t$ and with probability $\bar{p}$ if $\{i,j\}$ is not contained in any $C_t$. We let $O\subseteq [n]\times [n]$ be the set of pairs of vertices which are not simultaneously contained in any cluster. 

For an integer $N$ let $B_1,\dots, B_N$ be an independent sample of $N$ graphs with the distribution of $B$. Let $A^*$ be the $n\times n$ matrix with entries in $\{0,1\}$ which captures the underlying cluster structure, namely $A^*_{ij}=1$ iff there is a cluster $C_t$ which contains both $ij$. In this section we study the probability, as a function of $\delta$ that the optimization problem $\min_A\Delta(A)$
\[\Delta(A)= \delta\|2A-11^t\|_{*}+\frac{1}{N}\sum_{k=1}^N\|A-B_k\|_1\] 
has the correct cluster structure $A^*$ as a minimizer. For vertices $i,j\in [n]$ define $n_1(ij)$ (resp. $n_0(ij)$) the random variables which count the number of times that a given pair is (resp is not) an edge of some $B_j$, $j=1,\dots N$. Note that the $n_q(ij)$ for $q=0,1$ are binomial random variables.  

\begin{lemma} The following statements hold:
\label{lem: subdiff}
\begin{enumerate}
\item The subdifferential of $\frac{1}{N}\sum_{k=1}^N\|A-B_k\|_1$ at $A^*$ is the set of symmetric matrices $C$ satisfying the inequalities
\[ \frac{n_0(ij)-n_1(ij)}{N}\leq C_{ij}\leq 1 \text{, if $\{i,j\}\subseteq C_t$ for some $t$ and}\]
\[-1\leq C_{ij} \leq \frac{n_0(ij)-n_1(ij)}{N} \text{ if $\{i,j\}$ does not belong to any cluster. } \]
\item The subdifferential of $\delta\|2A-11^t\|_{*}$ at $A^*$ is given by the set of symmetric matrices of the form $2\delta C$ where $C$ has spectral norm $\|C\|\leq 1$ and satisfies $\langle C, 2A-11^t\rangle = n$.

\end{enumerate}

\end{lemma}
\begin{proof} $(1)$ Since the subdifferential is additive it suffices to understand the subdifferential of the absolute value. If $i,j\in C_t$ then $A^*_{ij}=1$ and the entry $ij$ of the subdifferential of the sum at $A^*$ is $[-1,1]$ for each $B_i$ containing the edge and it is $1$ for each $B_i$ for which $(ij)$ is not an edge. If $i,j$ is not contained in any cluster then $A^*_{ij}=0$ and the entry $ij$ of the subdifferential of the sum at $A^*$ is $-1$ for each $B_i$ which contains the edge $ij$ and $[-1,1]$ for each $B_i$ which does not, proving the claim. $(2)$ It is easy to prove that the subdifferential of any norm $\|\bullet\|$ at a point $X$ is given by those $C$ for which the dual norm $\|C\|_*\leq 1$ and $\langle C,X\rangle =\|X\|$. Claim $(2)$ follows because $\|2A-11^t\|=Tr(2A-11^t)=n$ where the first equality holds since $2A-11^t$ is positive semidefinite.\mv{Esto es obvio con solo dos clusters (la matriz es $uu^t$ donde $u$ es el vector con $1$'s en un cluster y $-1$'s en el complemento pero hay que demostrarlo para tres o mas)}.
\end{proof}

\begin{lemma} If $\delta=0$ then 
\[\PP\{A^*\in\argmin\Delta \} =\prod_{ij\in O} \PP\{n_1(ij)\leq n_0(ij)\} \prod_{t=1}^k \left(\prod_{(ij)\in C_t}\PP\{n_0(ij)\leq n_1(ij)\}\right)\]
where $\PP\{n_0(ij)\leq n_1(ij)\}$ is given by the following formula 
\mv{Ejercicio para Daniel: Encontrar una f\'ormula, es la probabilidad de que haya mas caras que sellos en $N$ lanzamientos de una moneda trucada donde las probabilidades de la moneda dependen s\'olo de la arista $ij$}.
\end{lemma}
\begin{proof} The matrix $A^*$ is a minimizer of the above convex function if and only if its subdifferential at $A^*$ contains the matrix $0$. By part $(1)$ of the previous Lemma this occurs if and only if $\frac{n_0(ij)-n_1(ij)}{N}\leq 0$ for $(ij)$ in a cluster and 
$\frac{n_0(ij)-n_1(ij)}{N}\geq 0$ for $(ij)$ in $O$. Independence of the edges then implies the above formula.  
\end{proof}
\begin{remark} Could the set of minimizers be larger? If $A'$ has at least one entry $A_{ij}\in (0,1)$ the corresponding component in the subgradient is the constant $n_0(ij)-n_1(ij)$ and this equals zero with much smaller probability, precisely when both terms equal to $\frac{N}{2}$. In particular it is impossible if $N$ is odd and in this case $A^*$ is the only minimizer.
\end{remark}

Next we ask whether it is possible to increase the probability of correct recovery by allowing $\delta>0$. By Lemma~\ref{lem: subdiff} $A^*\in \argmin \Delta$ if and only if there exists $S$ in the subdifferential of $\delta\|2A-11^t\|_*$ at $A=A^*$ such that $-S$ belongs to the subdifferential of $\frac{1}{N}\sum_{k=1}^N\|A-B_k\|_1$ at $A^*$. 

The most immediate way to do this would be to find an $S$ with $S_{ij}\leq 0$ negative on edges $ij\in C_t$ and $S_{ij}\geq 0$ on edges of $O$. More precisely we would like to find a best such $C$ by solving the optimization problem:

\[
\min \left(\sum_{t}\sum_{(ij)\in C_t} C_{ij}\right)-\sum_{(ij)\in O} C_{ij} \text{ s.t. $\|B\|\leq 1$, $\langle C, 2A^*-11^t\rangle = n$} 
\]

However it is easy to see that we cannot do this improvement simultaneously in all components, because $n=\langle S, 2A-11^t\rangle = Tr(S)+\sum_{ij \in O^c} S_{ij} -\sum_{ij \in O} S_{ij}\leq n+u$ where $u$ is the objective function in the problem above. We conclude that $u$ must be nonnegative so we cannot improve simultaneously in all directions at once.
In the following section we discuss how tradeoffs between components explain the improved recovery probability induced by the spectral norm.

\section{Estimating recovery probabilities}

Let $\Gamma$ be the symmetric matrix with zero diagonal and off-diagonal entries given by
$\Gamma_{ij}=\frac{n_0(ij)-n_1(ij)}{N}$. By Lemma~\ref{lem: subdiff} a symmetric matrix $C$ lies in the subdifferential if and only if it satisfies the inequalities
\[ \Gamma_{ij}\leq C_{ij}\leq 1 \text{, if $\{i,j\}\subseteq C_t$ for some $t$ and}\]
\[-1\leq C_{ij} \leq \Gamma_{ij} \text{ if $\{i,j\}$ does not belong to any cluster. } \]
To simplify these inequalities we define a linear operator $\widetilde{\bullet}$ on symmetric matrices by the formula
\[ \widetilde{A} = 
\begin{cases}
A_{ij}\text{ if $i=j$ or $ij\in I$ and}\\
-A_{ij}\text{ if $ij\in O$.}
\end{cases}
\]
in this language $C_{ij}$ belongs to the subdifferential if and only if $\widetilde{\Gamma}_{ij}\leq \widetilde{C}_{ij}$ for $i\neq j$.
The following key result gives sufficient conditions for the true cluster structure $A^*$ to be a minimizer of the proposed optimization problem. In order to describe it we introduce the following notation. 

\begin{definition} Let $\delta$ be a positive real number. For a symmetric matrix $\Gamma$ define the quantities
\[b(\Gamma,\delta):=\sum_{i\neq j} \max\left(\widetilde{\Gamma_{ij}}+\frac{2\delta}{n},0\right)
\text{ and } a(\Gamma,\delta):=\sum_{i\neq j} \max\left(-\widetilde{\Gamma_{ij}}-\frac{2\delta}{n},0\right)
\]\end{definition}
The quantity $b(\Gamma,\delta)$ (resp. $a(\Gamma,\delta)$) measures the total amount by which the matrix $\widetilde{-\frac{2\delta}{n}11^t}$ fails (resp. succeeds) to be in the subdifferential of Lemma~\ref{lem: subdiff} in the sense that it sums over all $ij$ the amount by which the inequalities $\widetilde{\Gamma_{ij}}\leq \frac{2\delta}{n} 11^t$ fail (resp. succeed). The key point of the following Theorem is that if the inequality fails by less than it succeeds then the subdifferential of the spectral norm is sufficiently rich so as to allow us to redistribute these quantities. In this sense the following Theorem explains the success of the spectral norm in cluster recovery algorithms. 


\begin{theorem}\label{thm: transport} Assume there are only two clusters. If $b(\Gamma,\delta)\leq \min(\delta, a(\Gamma,\delta))$ then $A^*$ is a minimizer of the optimization problem $\min_A\Delta(A)$. 
\end{theorem}
\begin{proof} We will show that there exists a matrix $C_{ij}$ such that $-C_{ij}\in \partial \left(\delta\|2A-11^t\|\right)(A^*)$ for which $\widetilde{\Gamma_{ij}}\leq \widetilde{C_{ij}}$ for $i\neq j$. It will then follow that $0=C-C$ belongs to the subdifferential of $\Delta(A)$ at $A^*$ and thus $A^*$ is a minimizer as claimed.

Recall that $-C\in \partial \left(\delta\|2A-11^t\|\right)(A^*)$ if and only if it satifies the conditions
\[ \langle H^* , C\rangle =-2\delta n\text{ and }\|C\|\leq 2\delta \]
where $H^*:=2A^*-11^t$ and $\|\bullet\|$ is the spectral norm. 
Both of these conditions are satisfied by setting $C=-\frac{2\delta}{n} H^*$. However this choice of $C$ will not, in general, satisfy the inequalities $\widetilde{\Gamma}_{ij}\leq \widetilde{-\frac{2\delta}{n} H^*}_{ij}=-\frac{2\delta}{n}11^t_{ij}$ for $i\neq j$. 

We will adjust our candidate for $\widetilde{C}$ by adding to it a transportation matrix $\widetilde{K}$ that will guarantee that all these inequalities are satisfied when $b(\Gamma,\delta)\leq a(\Gamma, \delta)$. Crucially we will choose $\widetilde{K}$ so that $K$ satisfies $KH^*=H^*K=0$ allowing us to control the spectral norm of $C$.

Since $b(\Gamma,\delta)\leq a(\Gamma,\delta)$ there exists a way to redistribute the quantity $b(\Gamma,\delta)$ by substracting it from the $ij$ for which $-\frac{\delta}{n}\leq \widetilde{\Gamma}_{ij}$  and adding it into those $st$ for which $\widetilde{\Gamma}_{st}\leq -\frac{\delta}{n}$. More specifically, if $b=\widetilde{\Gamma_{ij}}+\frac{2\delta}{n}>0$ then there exists a set $(i_1,j_1),\dots, (i_t,j_t)$ of non-diagonal entries and nonnegative constants $\gamma_{i_s,j_s}$ summing to one 
such that $\widetilde{\Gamma}_{i_sj_s}+\frac{2\delta}{n}+ b\gamma_{i_s,j_s}\leq 0$. Define the off-diagonal elements of $\widetilde{C}$ by
\[\widetilde{C}:=-\frac{2\delta}{n} 11^t + \sum_{i_s,j_s} b\gamma_{i_s,j_s}(-e_{ij}+e_{i_s,j_s})\]
where $e_{ij}$ is the symmetric matrix with one in positions $i,j$ and $j,i$ and zeroes otherwise. More generally, let $B$ be the set of paris $(i,j)$ with $i<j$ such that $\widetilde{\Gamma_{ij}}+\frac{2\delta}{n}>0$. If $b(\Gamma,\delta)\leq a(\Gamma,\delta)$ then for each $ij\in B$ there exist nonnegative constants $\gamma^{(ij)}_{st}$ such that $\sum_{s < t} \gamma^{(ij)}_{st}=1$ and for which the matrix
\[\widetilde{C}:=-\frac{2\delta}{n}11^t+\sum_{ij\in B} \sum_{s<t} \left(\widetilde{\Gamma_{ij}}+\frac{2\delta}{n}\right)\gamma_{st}^{(ij)} (-e_{ij}+e_{st})\]  
satisfies $\widetilde{\Gamma}_{ab}\leq \widetilde{C}_{ab}$ for all $a\neq b$. We will show that if $b(\Gamma,\delta)\leq 2\delta$ then the matrix $C$ with off-diagonal entries given by
\[C_{ab}=-\frac{2\delta}{n}H^*_{ab} + \sum_{ij\in B} \sum_{s<t} \left(\widetilde{\Gamma_{ij}}+\frac{2\delta}{n}\right)\gamma_{st}^{(ij)} \widetilde{(-e_{ij}+e_{st})_{ab}}\]
lies in $-\partial \left(\delta\|2A-11^t\|_*\right)(A^*)$ for some choice of diagonal, proving the Theorem. 
For a pair of indices $i<j$ let $\epsilon_{ij}=1$ if $ij\in I$ and $\epsilon_{ij}=-1$ if $ij\in O$. Define the matrix $t_{ij}$ by $t_{ij}=\epsilon_{ij} e_{ij}-e_{ii}-e_{jj}$ and note that $t_{ij}$ satisfies the following three properties: 
\begin{enumerate}
\item The equalities $H^*t_{ij}=0=t_{ij}H^*$ hold. If $ij\in O$ this happens only when there are exactly two clusters and this is the only point in the proof where this assumption is used.
\item The off-diagonal entries of $\widetilde{t_{ij}}$ are equal to those of $e_{ij}$. In particular the off-diagonal entries of $\widetilde{(-e_{ij}+e_{st})_{ab}}$ are always equal to those of $\widetilde{-t_{ij}+t_{st}}$.
\item The inequality $\|-t_{ij}+t_{st}\|\leq 2$ holds for all $ij$ and $st$. This is immediate via direct calculation (the inequality is strict only if $|\{i,j\}\cap\{s,t\}|\geq 1$ and in this case it can take values of $\sqrt{3}$ and $0$).  
\end{enumerate}
If $C$ denotes the matrix given by
\[C:= -\frac{2\delta}{n}H^* + \sum_{ij\in B} \sum_{s<t} \left(\widetilde{\Gamma_{ij}}+\frac{2\delta}{n}\right)\gamma_{st}^{(ij)} (-t_{ij}+t_{st})\]
then the following properties hold:
\begin{enumerate}
\item The off-diagonal entries agree with those in our previous expression so $\widetilde{\Gamma}_{ab}\leq \widetilde{C}_{ab}$ for all $a\neq b$ and thus $C$ is in the subdifferential of Lemma~\ref{lem: subdiff}.
\item Since $H^*t_{ij}=0=t_{ij}H^*$ the equality $\langle H^*, C\rangle =\langle H^*,-\frac{2\delta}{n}H^*\rangle = -2\delta n$ holds and moreover
\[ \| C\|=\max\left(\left\|-\frac{2\delta}{n}H^*\right\|, \left\|\sum_{ij\in B} \sum_{s<t} \left(\widetilde{\Gamma_{ij}}+\frac{2\delta}{n}\right)\gamma_{st}^{(ij)} (-t_{ij}+t_{st})\right\|\right).\] 
\end{enumerate}
The operator norm of the first term in the maximum equals $2\delta$ and that of the second term is bounded by $2b(\Gamma,\delta)$ by the triangle inequality and the definition of $b(\Gamma,\delta)$. We conclude that $\|C\|$ is bounded by $2\delta$ because $b(\Gamma,\delta)\leq \delta$. As a result $-C\in \partial\left(\|2A-11^t\|\right)(A^*)$ proving the Theorem.
\end{proof}

\mv{Como extendemos este razonamiento al caso de tres o m\'as clusters? Debe ser posible utilizar otras matrices de transporte para este caso que vivan en el kernel. Algo interesante es que con tres clusters es necesario que los promedios de una matriz aniquilada por $H^*$ dentro de cada uno de los bloques $C_i\times C_j$ deban ser cero.} 

Using the previous Theorem we now estimate the probabilities of perfect recovery of the correct cluster structure. 

\begin{theorem} Suppose $B_1,\dots, B_N$ are i.i.d. random graphs with independent entries. If the quantities $M$ and $J$ are given by the formulas 
\[ M:=\EE\left[\sum_{i\neq j}\widetilde{\Gamma_{ij}}\right]\text{ and }J:=\EE\left[\sum_{i\neq j}\max\left(\widetilde{-Z_{ij}^{(t)}}+\frac{2\delta N}{n},0\right)\right]\]
then the probability that $A^*\in \argmin \Delta(A)$ is bounded below by
\[1-\left(\exp\left( -\frac{N}{2}\left(1-\frac{1}{n}\right) \left(\frac{M}{n-1}-2\delta\right)^2\right)+\exp\left(-\frac{N(\delta-J)^2}{2n(n-1)}\right)\right)\]
whenever $\delta$ satisfies the inequalities $\frac{M}{2(n-1)}>\delta>J$. 
\end{theorem}

\begin{proof} By Theorem~\ref{thm: transport} the probability that $A^*\in \argmin \Delta(A)
$ is bounded below by $\PP\{b(\Gamma,\delta)\leq \min(\delta,a(\Gamma,\delta)\}$. By the union bound this probability is bounded below by the quantity
\[1-\left(\PP\{b(\Gamma,\delta)\geq a(\Gamma,\delta)\}+ \PP\{b(\Gamma,\delta)\geq \delta\}\right).\]
We will find upper bounds for the summands in the parenthesis proving the Theorem. Our key tool will be the following version of Hoeffding's inequality: If $X_1,\dots, X_T$ are independent random variables with values in $[c_i,d_i]$ and $\Lambda_T:=\sum_{i=1}^T X_i$ then the following inequality holds for any $t\geq 0$ 
\[\PP\{\Lambda_T-\EE[\Lambda_T]\geq t\}\leq \exp\left(-\frac{2t^2}{\sum_{i=1}^T (d_i-c_i)^2}\right).\]
To obtain a bound for the first term, define for $t=1,\dots, N$ the random variable $Z^{(t)}:=2B_t-11^t$ and note that for every $i\neq j$ the equality
\[\sum_{t=1}^N-\frac{Z_{ij}^{(t)}}{N} = \frac{n_0(ij)-n_1(ij)}{N}=\Gamma_{ij}\]
holds. As a result
\[b(\gamma,\delta)-a(\Gamma,\delta) = \sum_{i\neq j} \left(\widetilde{\Gamma_{ij}}+\frac{2\delta}{n}\right)= \frac{2\delta n(n-1)}{n} + \sum_{t=1}^N \sum_{i\neq j} \frac{-\widetilde{Z^{(t)}_{ij}}}{N}\]
and therefore if $M:=\EE\left[\sum_{t=1}^N \sum_{i\neq j} \frac{\widetilde{Z^{(t)}_{ij}}}{N}\right]=\EE\left[\sum_{i\neq j} \widetilde{Z^{(t)}_{ij}}\right]$ then 
\[\PP\left\{\frac{2\delta n(n-1)}{n} + \sum_{t=1}^N \sum_{i\neq j} \frac{-\widetilde{Z^{(t)}_{ij}}}{N}\geq 0\right\} = \PP\left\{\sum_{t=1}^N \sum_{i\neq j} \frac{-\widetilde{Z^{(t)}_{ij}}}{N} + M \geq -2\delta (n-1) + M \right\}.\]

Estimating the last term via Hoeffding's inequality with $Nn(n-1)$ summands with values in $\left[-\frac{1}{N},\frac{1}{N}\right]$ we conclude that 
\[\PP\{b(\Gamma,\delta)\geq a(\Gamma,\delta)\}\leq \exp\left(-2\frac{(M-2\delta(n-1))^2}{Nn(n-1)(\frac{2}{N})^2}\right)=\exp\left( -\frac{N}{2}\left(1-\frac{1}{n}\right) \left(\frac{M}{n-1}-2\delta\right)^2\right) \]
whenever $\frac{M}{n-1}-2\delta>0$.

For estimating the second term, recall that
\[b(\Gamma,\delta)= \sum_{i\neq j} \max\left(\widetilde{\Gamma_{ij}}+\frac{2\delta}{n},0\right) = \sum_{i\neq j} \max\left(\sum_{t=1}^N\frac{\widetilde{-Z_{ij}^{(t)}}+\frac{2\delta N}{n}}{N},0\right)\]
By Jensen's inequality applied to the convex function $\max(\bullet,0)$ the last term can be bounded above as
\[
\sum_{i\neq j} \max\left(\sum_{t=1}^N\frac{\widetilde{-Z_{ij}^{(t)}}+\frac{2\delta N}{n}}{N},0\right)\leq \sum_{t=1}^N \sum_{i\neq j}  \frac{1}{N} \max\left(\widetilde{-Z_{ij}^{(t)}}+\frac{2\delta N}{n},0\right)
\]
As a result
\[\PP\{b(\Gamma,\delta)\geq \delta\}\leq \PP\left\{ \sum_{t=1}^N \sum_{i\neq j}  \frac{1}{N} \max\left(\widetilde{-Z_{ij}^{(t)}}+\frac{2\delta N}{n},0\right)\geq \delta\right\}\]
And we can estimate the last term via Hoeffding's inequality with $Nn(n-1)$ summands taking values in the interval $\left[-\frac{1}{N}+\frac{2\delta}{n},\frac{1}{N}+\frac{2\delta}{n}\right]$ obtaining

\[\PP\{b(\Gamma,\delta)\geq \delta\}\leq \exp\left( -\frac{2(\delta-J)^2}{Nn(n-1)\left(\frac{2}{N}\right)^2}\right)=\exp\left(-\frac{N(\delta-J)^2}{2n(n-1)}\right)\]
whenever $J:=\EE\left[\sum_{i\neq j}\max\left(\widetilde{-Z_{ij}^{(t)}}+\frac{2\delta N}{n},0\right)\right]$ satisfies $J<\delta$.
\end{proof}

\mv{Esta prueba todav\'ia no muestra lo que queremos. El problema es que la cantidad $J$ depende de $N$. Esto esta causado por intentar hacer aparecer $N$, usando la desigualdad de Jensen obteniendo una suma con $Nn(n-1)$ t\'erminos. El precio es que esos t\'erminos terminan con valor esperado dependiente de $N$. Creo que una soluci\'on posible ser\'ia usar una desigualdad m\'as precisa que Hoeffding que involucre la varianza, por ejemplo la desigualdad de Bernstein (porque el punto es que la varianza de $\max(\widetilde{\Gamma_{ij}}+\frac{2\delta}{n},0)$ se va para cero cuando $N$ crece). Se logra as\'i una buena desigualdad?} 

\mv{Algunas ideas mas concretas. El problema es acotar por encima $\PP\{b(\Gamma,\delta)\geq \delta\}$. Ahora $b(\Gamma,\delta):=\sum_{i\neq j} \max\left(\widetilde{\Gamma_{ij}}+\frac{2\delta}{n},0\right)$ asi que es una suma de variables aleatorias independientes, cada una en el rango $[0, 1+ \frac{2\delta}{n}]$. Lo importante es que en la medida en que $N$ crece estas variables aleatorias deben estar cada vez m\'as concentradas en $0$. Para cuantificar esto necesitamos entender (i.e. encontrar una cota superior simple como funcion de $N$) para alguno de los siguientes 
\begin{enumerate}
\item Entender el valor esperado para cada sumando.
\item Entender la varianza. (con estos dos podriamos utilizar la desigualdad de Bennett para obtener el resultado, ver notas de Pollard) 
\item Entender la funci\'on generadora de momentos. Esta es la clave para una eventual desigualdad de concentraci\'on.
\end{enumerate}
}



\begin{cor} Given $\delta<0$ the probability that $A^*$ is a minimizer of the optimization problem $\min_A\Delta_{\delta}(A)$ is bounded below by $\mathbb{P}\{ b(\Gamma,\delta)\leq \min \left(a(\Gamma,\delta), 2\delta\right)\}$.  
This quantity can be computed in terms of the joint distribution of two sums over all edges of independent random variables, because the following idetities hold:
\[b(\Gamma,\delta)+a(\Gamma,\delta)=\sum_{ij} \left|\widetilde{\Gamma}_{ij}+\frac{2\delta}{n}\right|\]
\[b(\Gamma,\delta)-a(\Gamma,\delta)=\sum_{ij} \left(\widetilde{\Gamma}_{ij}+\frac{2\delta}{n}\right)\]
and in particular, 
\[\mathbb{P}\{ b(\Gamma,\delta)\leq \min \left(a(\Gamma,\delta), \delta\right)\}\geq \]  
\mv{TAREA: Encontrar la mejor f\'ormula posible. Esto debe poder hacerse usando desiguladades de concentraci\'on junto con las igualdades del Corolario. Notar que esta desigualdad es independiente de la prueba del Teorema anterior y es solo un problema de probabilidad multinomial.}
\end{cor}






\end{document}
Note that $\Gamma$ is the set of variable endpoints in the inequalities of Lemma~\ref{lem: subdiff}. Accor










Our following Lemma gives us a decomposition Theorem for symmetric matrices. This decomposition will be very useful when computing spectral norms in the two-block case.
Suppose that we have only two blocks $C_1$ and $C_2$ and let $u$ be the vector given by
\[u_i=\begin{cases}
1\text{ if $i\in C_1$}
-1\text{ if $i\in C_2$}
\end{cases}
\]
Define $H^*:=uu^t$ and for $i=1,\dots, n$ let $e_{ii}$ be the $n\times n$ matrix whose only nonzero entry is in position $ii$ and has value $1$ 


\begin{lemma} Assume $|C_1|\neq |C_2|$ If $A$ is a symmetric matrix then there exist unique constants $\lambda_1,\dots, \lambda_{n-1}$ and a matrix $K$ such that $KH^*=H^*K=0$ such that
\[A= \frac{\langle H^*,A\rangle }{n^2} H^* +\sum_{i=1}^{n-1}\lambda_1(e_{i,i}-e_{i+1,i+1})+K.\]
And in particular the following equality holds  
\[ \|A\| =\max(A-K,K).\]
\end{lemma}
\begin{proof} The $n$ vectors in $v^tH^*$ and $v^t(e_{i,i}-e_{i+1,i+1})$ for $i=1,\dots, n-1$ are linearly independent in $\RR^n$ because the sum of the entries of all but the first one vanish. It follows that $v^tA$ can be expressed uniquely as a linear combination of them 


\end{proof}






\end{document}


