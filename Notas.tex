\documentclass[12pt]{amsart}
\usepackage{fullpage,url,amssymb,amsmath,graphicx}

\usepackage{amsmath, amsthm, amssymb}
\usepackage{url}
\usepackage{graphicx}
\usepackage {amsmath}
\usepackage{amsthm}
\usepackage {amssymb}
\usepackage{dsfont}

%\usepackage{setspace}
%\documentclass[12pt]{amsart}
%\documentclass[letterpaper,10pt]{article}
%\documentclass[letterpaper,12pt]{article}
\setlength{\oddsidemargin}{0.15in} \setlength{\evensidemargin}{0cm}
\setlength{\marginparwidth}{28mm}
\setlength{\marginparsep}{28mm}
\setlength{\marginparpush}{25mm}
\setlength{\topmargin}{0in}
\setlength{\headheight}{0pt}
\setlength{\headsep}{15mm}    
\setlength{\textheight}{21cm}
\setlength{\textwidth}{6.0in}
\setlength{\parskip}{4pt}

\usepackage{amsmath}
%\usepackage{amsthm}
\usepackage {amssymb}
%\usepackage {stmaryrd}
\usepackage {graphicx,enumerate,booktabs}
\usepackage {color, tikz}
%\usepackage{graphics}
\usepackage[all]{xy}
%\usepackage{amsrefs}


\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}[section]
\newtheorem{theorem}[lemma]{Theorem}
\newtheorem{propo}[lemma]{Proposition}
\newtheorem{cor}[lemma]{Corollary}
\newtheorem{conj}[lemma]{Conjecture}
\newtheorem{claim}[lemma]{Claim}
\newtheorem{claim*}{Claim}
\newtheorem{remark}[lemma]{Remark}
\newtheorem{quest}[lemma]{Question}
\newtheorem{example}[lemma]{Example}


\numberwithin{equation}{section}


\newcommand{\Cox}{\operatorname{Cox}}
\newcommand{\Pic}{\operatorname{Pic}}
\newcommand{\Tor}{\operatorname{Tor}}
\newcommand{\diam}{\operatorname{diam}}
\newcommand{\Spec}{\operatorname{Spec}}
\newcommand{\Osh}{{\mathcal O}}
\newcommand{\kk}{\kappa}
\newcommand{\Hom}{\operatorname{Hom}}
\newcommand{\C}{\operatorname{Cone}}
\newcommand{\Sym}{\operatorname{Sym}}
\newcommand{\AC}{\mathcal{AC}}
\newcommand{\MC}{\mathcal{MC}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\supp}{\operatorname{supp}}
\newcommand{\D}{{\mathcal D}}

\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\PP}{\mathbb{P}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\FF}{\mathbb{F}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\EE}{\mathbb{E}}

%\parindent = 0pt

\newcommand{\mv}[1]{{\color{red} \sf $\clubsuit\clubsuit\clubsuit$ Mauricio: [#1]}}

\newcommand{\ddr}[1]{{\color{blue} \sf $\clubsuit\clubsuit\clubsuit$ Daniel: [#1]}}


\usepackage{tikz}

\definecolor{cof}{RGB}{219,144,71}
\definecolor{pur}{RGB}{186,146,162}
\definecolor{greeo}{RGB}{91,173,69}
\definecolor{greet}{RGB}{52,111,72}

\begin{document}

\section{Part 1}

{\bf Problem (Robust PCA).} Find $(A^*, B^*)={\rm argmin} \gamma \|A|_1+\|B\|_*$
subject to $A+B=C$ where $C$ is a given matrix. Here $\|B\|_*=\sum \sigma_i(B)$
where $\sigma_i(B)$ are the square roots of the eigenvalues of the matrix $B^tB$.

\begin{lemma} The nuclear norm has the following properties:
\begin{enumerate}
\item It is a semidefinitely representable function
\item It is the dual of the spectral norm. 
\item Evaluation of the nuclear norm can be done via a semidefinite programming problem.
\[\sup_Y Tr(X^tY) \text{ subject to}
\left(
\begin{array}{cc}
I_n & Y\\
Y^t & I_m\\
\end{array}
\right)\succeq 0
\] 


\item The dual of this problem is:
\[
\inf{\rm Tr}(Z) \text{ subject to }Z=\left(
\begin{array}{cc}
Z_{11} & \frac{X}{2}\\
\frac{X}{2} & Z_{22}\\
\end{array}
\right)\succeq 0
\]

\end{enumerate}
\end{lemma}
\begin{proof} We will:
\begin{enumerate}
\item Prove that the spectral norm $\|Y\|$ is SDR. We use the notation $\|Y\|$ to denote the spectral norm.
\item Compute the dual of the spectral norm and prove that it IS the nuclear norm. 
\end{enumerate}


Recall that $\|Y\|=\min t$ such that 
\[\left(
\begin{array}{cc}
tI_n & Y\\
Y^t & tI_n\\
\end{array}
\right)\succeq 0
\]
Obviously $\|A\|_2\leq t$ if and only if $A^tA\preceq t^2I$. The {\bf Schur complement} of the above matrix is $S=tI-Y^t(tI)^{-1}Y=tI-\frac{1}{t}A^tA$ so the claim follows by the Schur complement Lemma. We have in fact proven that the spectral norm is a semidefinitely representable function.


Recall that if $\|\cdot\|$ is a norm we define its dual norm $\|x\|_d:=\sup_{\|y\|\leq 1} \langle x,y\rangle$ (properties: involutive and $|\langle x,y\rangle\leq \|x\|\|y\|_d$). 

{\bf Obs.} The dual norm of the spectral norm is equal to the sum of the singular values.
{\bf proof.} It is true that the equalities $\|Y\|=\|\Sigma\|$ and $\|Y\|_d=\|\Sigma\|_d$ hold where $\Sigma$ is the diagonal matrix of the singular values of $Y$. As a result $\|\Sigma\|_d=\sup_{\|Z\|\leq 1} Tr(\Sigma^tZ)$. Choosing $Y=Id$ we see that $\|Y\|_d\geq Tr(\Sigma)=\|Y\|_*$ Moreover, if a matrix $Z$ satisfies $\|Z\|\leq 1$ then $|Z_{11}|\leq \|Ze_1\|_2\leq \|Z\|\leq 1$. As a result $Tr(\Sigma^tZ)\leq Tr(\Sigma)$ proving the claim.

$(3)$ Given a matrix $X$ we compute $\|X\|_d$ as:
\[\sup_Y Tr(X^tY) \text{ subject to}
\left(
\begin{array}{cc}
I_n & Y\\
Y^t & I_m\\
\end{array}
\right)\succeq 0
\] 

We compute the Lagrangian dual and obtain the problem above. Strong duality holds because Slater's constraints are satisfied (since there is a point in the strict interior of our cone). 
\end{proof}

As a result we can formulate the robust PCA optimization problem
{\bf Problem (Robust PCA).} Find $(A^*, B^*)={\rm argmin} \gamma \|A|_1+\|B\|_*$
subject to $A+B=C$ where $C$ is a given matrix. Here $\|B\|_*=\sum \sigma_i(B)$
where $\sigma_i(B)$ are the square roots of the eigenvalues of the matrix $B^tB$ as:


\[\min_{A,B,W_1,W_2,Z} \gamma 1^tZ1+{\rm Tr}(W)\\
\text{ subject to }W=\left(
\begin{array}{cc}
W_{11} & \frac{B}{2}\\
\frac{B}{2} & W_{22}\\
\end{array}
\right)\succeq 0,
-Z_{ij}\leq A_{ij}\leq Z_{ij},
A+B=C
\]


{\it De d\'onde sale la norma nuclear?}

Recall that if $f:C\rightarrow \RR$ the convex hull of the function is the biggest convex function $g$ such that $g(x)\leq f(x)$. $g(x)=f^{**}(x)$ where $f^*(y):={\rm sup_{x\in C}}\{\langle x,y\rangle -f(x)\}$

Let $C$ be a convex set and let $f:C\rightarrow \RR$ be a function. The function $g(x)$ is the best convex surrogate of $f(x)$ if $g(x)\leq f(x)$, $g(x)$ is convex and for every convex function $q(x)$ with $q(x)\leq f(x)$ the inequality $q(x)\leq g(x)$ holds.

Recall that $f^*(y)=\sup\{\langle y,x\rangle -f(x)\}$. The main properties are summarized in the following Theorem,

\begin{theorem} The following properties hold:
\begin{enumerate}
\item {\bf Defining property} The affine function $\langle x,y\rangle-f^*(y)$ is a lower bound for $f$, that is
\[\forall x\left(\langle x,y\rangle-f^*(y)\leq f(x) \right)\] 
\item {\bf Computation of convex surrogates} The convex surrogate is, by definition the supremum of all affinel linear functions which bound $f(x)$ from below. As a result
\[g(x)=\sup_{y}\left(\langle x,y\rangle -f^*(y)\right)\]
which by definition agrees with $f^{**}(x)$.
\end{enumerate}
\end{theorem}



\begin{lemma} The nuclear norm is the best convex surrogate of the rank function in the set of matrices with $\|X\|\leq 1$.
\end{lemma}
\begin{proof} IF $\phi(X)$ is the rank then we compute
$\phi^*(X)={\rm sup}_{\|X\|\leq 1} \{\langle Y,X\rangle -\phi(X)\}$.

By Von Neumann's inequality ${\rm Tr}(YX^t)\leq \sum_{i=1}^ n \sigma_i(X)\sigma_i(Y)$ and equality holds if and only if $X$ and $Y$ admit a simultaneous singular values diagonaliation. We conclude that
\[\phi^*(Y)={\rm sup}_{\|X\|\leq 1} \{\sum \sigma_i(X)\sigma_i(Y)-\phi(X)\}\]

where the supremum is taken over $X$'s which are diagonal when $U$ is and have operator norm (i.e. absolute value) at most one. Since the $\ell_1$-norm is dual to the $\ell^{\infty}$ norm It follows that

\[\phi^*(X)=\max_{1\leq r\leq \min(m,n)}-r+\sum_{i=1}^r\sigma_i(X)=\sum_{i=1}^r\max(0, \sigma_i(Y)-1)\]

 

\end{proof}

\section{Matrix decomposition as sums of low-rank and sparse matrices}

Let $n,k,m$ be positive integers. A decomposition of a square $n\times n$ matrix $C$ is an expression of the form $C=A^*+B^*$ where $A^*$ has low rank (at most $k$) and $B^*$ is sparse (having at most $m$ nonzero components). Let $R_k$ be the variety of matrices of rank $\leq k$ and let $S_m$ be the (reducible) variety of matrices having at most $m$ non-zero components.


\subsection{Tangent space identifiability vs. actual identifiability}

It is claimed in Section $3.2$ of~\cite{IncoherenceDecomposition} that if $C$ admits a decomposition $C=A^*+B^*$ then this decomposition is unique if and only  $T_{A^*}(R_{k})\cap T_{B^*}(S_m)=0$. In this section we show that this claim is not true. However as we will show next, there is a valid {\it local version}. 

{\bf Counterexample.} Let $n=2$ we will find several distinct decompositions of a $2\times 2$ matrix as a sum of a matrix of rank one and a $1$-sparse matrix and verify that transversality of the tangent spaces holds at at least one of these.
\[
\left(
\begin{array}{cc}
1 & 2\\
3 & 4\\
\end{array}
\right)=
\left(
\begin{array}{cc}
\frac{3}{2} & 2\\
3 & 4\\
\end{array}
\right)
+\left(
\begin{array}{cc}
-\frac{1}{2} & 0\\
0 & 0\\
\end{array}
\right)
\]
\[
\left(
\begin{array}{cc}
1 & 2\\
3 & 4\\
\end{array}
\right)=
\left(
\begin{array}{cc}
1 & \frac{4}{3}\\
3 & 4\\
\end{array}
\right)
+\left(
\begin{array}{cc}
0 & \frac{2}{3}\\
0 & 0\\
\end{array}
\right)
\]
The variety of rank one matrices is given by 
\[R_1=\left\{
\left(\begin{array}{cc}
a & b\\
c & d\\
\end{array}
\right):ad-bc=0
\right\}
\]
as a result the tangent space at $A^*$ in the first decomposition is given by
\[T_{A^*}(R_1)=\left\{
\left(\begin{array}{cc}
\alpha & \beta\\
\gamma & \delta\\
\end{array}
\right):4\alpha-3\beta-2\gamma+\frac{3}{2}\delta=0
\right\}\]   
The tangent space $T_{B^*}(S_1)$ is given by $\beta=\gamma=\delta=0$ so the two tangent spaces intersect transversely.

\mv{Ejercicio. Demuestre que toda matriz de $n\times n$ se puede escribir de por lo menos $n^2$ maneras distintas como suma de una matriz $1$-sparse y una de rango $n-1$. Hay algun caso en que se pueda en m\'as de $n^2$ maneras? (Ayuda: Si, de hecho algunas matrices admiten infinitas maneras pero estas viven en alguna subvariedad del espacio de matrices).} 

However, tangent spaces are enough to guarantee that there are only finitely many decompositions (so that locally in open neighborhoods of $A^*$ and $B^*$ there is only one decomposition).

\begin{lemma} Let $M,N\subseteq \RR^n$ be differentiable submanifolds and let $p\in M$, $q\in N$. If $T_pM\cap T_qN=\{0\}$ then the map $\sigma: M\times N:\rightarrow \RR^n$ given by $\sigma(x,y)=x+y$ is a diffeomorphism between $M\times N$ and $\sigma(M\times N)$ near $(p,q)$. If, moreover $M$ and $N$ are algebraic varieties and $p,q$ are non-singular points in $M,N$ then $\sigma$ is generically finite. 
\end{lemma}
\begin{proof} The derivative of $\sigma$ at $(p,q)$ is the map
\[D\sigma: T_pM\times T_qN\rightarrow \RR^n\]
which maps vectors $(u,v)$ to $u+v$. The assumption of injectivity proves that the differential surjects onto the tangent space of the image because $\sigma(M\times N)$ has dimension at most $m+n$ and the image of $D\sigma$ has dimension at least $m+n$ by transversality of the tangent spaces \mv{Maybe we need $p,q$ to be generic points to make sure $\sigma(p,q)$ is a non-singular point of $\sigma(X\times Y)$}. In the algebraic case Chevalley's theorem on the dimension of fibers implies that the map $\sigma$ has at least one $zero$-dimensional fiber and therefore the generic fiber is finite, proving the claim.
\end{proof}

We conclude that the following theorem holds,

\begin{theorem} Suppose $A^*\in R_k$ and $B^*\in S_m$ have transverse tangent spaces. If these matrices are generic then the matrix $C:=A^*+B^*$ has only this decomposition near $A^*$ and $B^*$.
\end{theorem}


\begin{remark} If $M=R_k$, $N=S_m$ and $T_{A^*}(R_k)\cap T_{B^*}(S_m)=0$ then the Lemma says that the set $\sigma(R_k\times S_m)$ (i.e. the matrices that can be written as sums of a rank $\leq k$ and an $m$-sparse matrix) are locally parametrized by $R_k\times S_m$ via $\sigma$. In particular this map is injective in open neighborhoods near $A^*$ and $B^*$ in their respective varieties so $C$ has no other decomposition {\it in those neighborhoods}. As we saw in the counterexample this is NOT an obstacle for the existence of other decompositions far away. In more geometric terms the issue is that the generically finite map above does not have to have degree one.   
\end{remark}
\newpage

\section{Cluster learning}
Fix an integer $N$. By a cluster structure matrix on a graph in $[n]$ we mean a matrix $A$ which is the adjacency matrix of a disjoint union of cliques on $[n]$. In particular $A$ is a symmetric $\{0,1\}$-matrix and the rank of $A+I$ coincides with the number of clusters in $A$. Let $\mu$ be a probability measure in the set of graphs in $[n]$. We want to find a cluster structure $A$ which minimizes the expected cardinality of the symmetric difference between a random graph $G$ chosen according to $\mu$ and $A$. In other words the learning problem consists of finding a minimizer $A$ of the risk function
\[R(A):=\EE_B[\|A-B\|_1]\] 
where $B$ is the adjacency matrix of a graph sampled according to $\mu$.
The difficulty lies in the fact that all the information we have about the distribution of $B$ is given by an i.i.d sample $B_i$, $i=1,\dots,m$ of random variables distributed according to $\mu$.


\subsection{Learning and robust optimization}

A way to approximate our unknown measure $\mu$ is to replace it by its empirical measure $P_m:=\frac{1}{m}\sum_{i=1}^m\delta_{B_i}$. However, it will often be the case that $m$ is small (because of limitations in either the available data or the computational resources needed for the optimization) so we do not have guarantees that $\mu^{m}$ is, by itself, close to $\mu$. We therefore introduce an additional parameter $\delta$ and will assume that the actual distribution belongs to a ball of radius $\delta$ of the empirical measure.

\begin{definition} Endow the space $M$ of symmetric $n\times n$ matrices with a given matrix norm $\|\cdot\|_*$. If $\nu_1$ and $\nu_2$ are measures taking values on $M$ define the *-Wasserstein distance $W_*(\nu_1,\nu_2)=\inf_{\pi\in\mathcal{M}(K\times K):\pi_i=\nu_i}\EE[\|Z_1-Z_2\|_*]$ where $(Z_1,Z_2)$ is distributed according to $\pi$.
\end{definition}

%\begin{theorem} Let $C$ be the set of adjacency matrices of disjoint unions of complete subgraphs and let $V_{\delta}(\mu)=\{\nu: W_s(\nu,\mu)\leq \delta\}$ and let $P_m$ denote the empirical measure after $m$ samples. The following upper bound for the risk holds
%\[\min_{A\in C}\sup_{\mu\in V_{\delta}(P_m)} \EE[\|A-B\|_1]\leq \min_{A\in C} \frac{1}{m}\left(\sum_{i=1}^m \|A-B_i\|_1\right)+\delta\|2A-11^t\|_*\]
%\mv{Is this inequality an equality? Yo creo que si.}
%\end{theorem}
%\begin{proof} Suppose $B$ is distributed according to $\mu\in V_{\delta}(P_m)$ and that $B$ takes values in the set of adjacency matrices of graphs. Since both $A$ and $B$ are adjacency matrices of graphs the following equality holds
%\[\|A-B\|_1= \langle A, A-B\rangle + \langle 11^t-A,B-A\rangle=\langle 2A-11^t,A-B\rangle \]
%Next let $B^{(m)}$ be distributed according to the empirical measure $P_m$ centered in $m$ samples and that the joint distribution of $(B,B^{(m)})$ is given by a coupling $\pi$ which is a minimizer of the Wasserstein spectral norm $W_s(\mu,P_m)$. We then have

%\[\langle 2A-11^t,A-B\rangle = \langle 2A-11^t,A-B^{(m)}\rangle+\langle 2A-11^t,B^{(m)}-B\rangle\]
%the last quantity is bounded above by
%\[\langle 2A-11^t,A-B^{(m)}\rangle +\|2A-11^t\|_*\|B^{(m)}-B\|\]
%Taking expected value we obtain
%\[\frac{1}{m}\left(\sum_{i=1}^m \|A-B_i\|_1\right)+\|2A-11^t\|_*\EE\|B^{(m)}-B\|\]
%Since $\pi$ is a minimizer of $W_s(\mu,P_m)$  the rightmost expected value is bounded above by $\delta$. As a result we  conclude that for every $\mu\in V(\delta)$ the following inequality holds,
%\[\EE[\|A-B\|_1]\leq  \frac{1}{m}\left(\sum_{i=1}^m %\|A-B_i\|_1\right)+\delta\|2A-11^t\|_*\]
%as claimed.


%\end{proof}

%\mv{Pregunta: Qu\'e tal si regularizamos no sumando la norma nuclear de $A$ sino la de $\|2A-11^t\|_*$? Ayuda esto a recuperar si tenemos mas datos? Notar que en ese caso el par\'ametro $\delta$ parece mucho m\'as f\'acil de interpretar}.

%\mv{Esto se conecta muy muy bien con \url{https://arxiv.org/pdf/1610.05627.pdf}. Hay que leer este paper.
%Ver Secci\'on $3.2$ y proposici\'on $4$. Por ejemplo da esto nuevos algoritmos de c\'alculo?}

%\section{Learning and robust optimization 2}

Given an independent sample $B_1,\dots, B_N$ of random graphs on $[N]$ we wish to choose a symmetric matrix $A\in \{0,1\}^{n\times n}$ which minimizes the worst-case risk among all probability distributions in the ball of radius $\delta$ around the empirical measure $\hat{\mathbb{P}_N}$ determined by our sample. We assume that all our distributions take values in the hypercube $K$ consisting of adjacency matrices of graphs with entries in $[0,1]$. 

Let $||\cdot||_{*}$ be a given matrix norm a denote its dual norm by $||\cdot||_{d*}$ Our aim is to compute the quantity
\[R_{\delta}(A):=\sup_{\nu\in B_{\delta}(\hat{\mathbb{P}_N})}\EE[\|A-B\|_1]\]
where $B$ is distributed according to the measure $\nu$ and $B_\delta(\hat{\mathbb{P}}_N)$ is the ball of radius $\delta$ of the *-Wasserstein metric centered in the empirical measure $\hat{\mathbb{P}}_N$. We first reprove the following key Lemma due to Esfahani and Kuhn. We assume that the empirical measure $\hat{\mathbb{P}}_N$ is given by $\PP=\sum_{i=1}^Nc_i\delta_{B_i}$ where $c_i>0$ are constants which sum to one. 

\begin{theorem}
\label{teoError}
For every $A\in \{0,1\}^{n\times n}$ the number \[\alpha:=\sup_{\nu\in B_{\delta}(\hat{\mathbb{P}_N})} \EE[\|A-B\|_1]\]
is equal to the optimal value of the problem
\[\min_{H} \left(\lambda\delta +\sum_{i=1}^N c_is_i+\sum_{i=1}^Nc_i\|A-B_i\|_1\right)\]
where $H$ is the set of $(\lambda, s_1,\dots, s_N, W)\in \RR\times\RR^N\times \RR^{n\times n} \times \RR^{n\times n}$ which satisfy the inequalities
\begin{center}
\[
\begin{array}{l}
\|W\|_{d*}\leq \lambda\\
\Lambda \geq 0\\
2A-11^t-W + \Lambda \geq 0\\
\|\Lambda\|_{1}\leq s_i-\langle 2A-11^t-W, B_i\rangle\text{ for $i=1,\dots, N$}
\end{array}
\]
\end{center}

\end{theorem}

\begin{proof} Since $A$ has entries in $\{0,1\}$ and $B$ has entries in $K$ the equality $\|A-B\|_1=\langle 2A-11^t, A-B\rangle=\|A-\hat{B}\|_1+\langle 2A-11^t, \hat{B}-B\rangle$ where $\hat{B}$ is distributed according to the empirical measure. Taking expected values and suprema we see that
\[\alpha=\sum_{i=1}^N c_i\|A-B_i\|_1+\sup_{\nu\in B_{\delta}(\hat{\mathbb{P}_N})}\EE\left[\langle 2A-11^t, \hat{B}-B\rangle\right].\] 
Let $\mathcal{M}(K\times K,\hat{\mathbb{P}_N})$ be the set of probability measures in $K\times K$ whose marginal distribution in the first component is given by $\hat{\mathbb{P}_N}$. Every such measure is of the form 
\[\mu = \sum_{i=1}^N c_i \delta_{B_i}\otimes \mathbb{Q}_i\]
where $\mathbb{Q}_i$ is a probability measure on $K$. These measures act on functions on two sets of variables corresponding to each copy of $K$. More concretely
\[\int_{K\times K} f(x,y) d\mu = \sum_{i=1}^ N c_i \int_K f(B_i,y)d\mathbb{Q}_i(y).\]
This allows us to rewrite
\[\sup_{\nu\in B_{\delta}(\hat{\mathbb{P}_N})}\EE\left[\langle 2A-11^t, \hat{B}-B\rangle\right] = 
\sup_{\mu\in \mathcal{M}(K\times K, \hat{\mathbb{P}_N})} \inf_{\lambda\geq 0} \int_{K\times K}\langle 2A-11^t, X-Y\rangle d\mu +\lambda\left(\delta -\int_{K\times K} \|X-Y\|_*d\mu\right)\]
Exchanging sup and inf we obtain that this quantity is bounded above \mv{equals?}
\[\inf_{\lambda\geq 0} \left(\lambda \delta +\sup_{\mu\in \mathcal{M}(K\times K, \hat{\mathbb{P}_N})}\int_{K\times K} \langle 2A-11^t, X-Y\rangle -\lambda \|X-Y\|_*d\mu\right)\]
Using the fact that $\mu = \sum_{i=1}^N c_i \delta_{B_i}\otimes \mathbb{Q}_i$ this quantity equals
\[\inf_{\lambda\geq 0} \left(\lambda\delta + \sum_{i=1}^N c_i \sup_{\mathbb{Q}_i} \int_{K} \langle 2A-11^t, B_i-Y\rangle -\lambda \|B_i-Y\|_*d\mathbb{Q}_i\right)\]
Since $\mathbb{Q}_i$ are arbitrary probability measures this quantity equals
\[\inf_{\lambda\geq 0} \lambda \delta +\sum_{i=1}^N c_i \sup_{Y\in K} \left(\langle 2A-11^t, B_i-Y\rangle -\lambda \|B_i-Y\|_*\right)\]

Next we put the supremum into the constraints obtaining the equivalent problem
\[ \inf_{(\lambda,s_1,\dots, s_N)} \lambda\delta +\sum c_is_i\]
over the set given by the inequalities $\lambda\geq 0$ and for which the following inequalities in $\lambda$ are satisfied for $i=1,\dots, N$.
\[\sup_{Y\in K} \left(\langle 2A-11^t, B_i-Y\rangle -\lambda \|B_i-Y\|_*\right)\leq s_i\]


Next we compute the supremum in the constraints using strong duality \mv{Check hypothesis for strong duality}. We use duality because we are aiming to bound  the supremum above by $s_i$ and for this it suffices to exhibit a point in the dual whose objective function is bounded above by $s_i$. 

\[\sup_{Y\in K} \left(\langle 2A-11^t, B_i-Y\rangle -\lambda \|B_i-Y\|_*\right)=
\sup_{Y\in K} \inf_{W: \|W\|_{d*}\leq \lambda} \langle 2A-11^t-W, B_i-Y\rangle  =\]
\[=\inf_{W: \|W\|_{d*}\leq \lambda}\sup_{Y\in K}\langle 2A-11^t-W, B_i-Y\rangle 
%=\inf_{W: \|W\|_*\leq \lambda} \langle 2A-11^t-W, B_i\rangle+ \sup_{Y\in K} \langle D,-Y \rangle.
\]
By the comment above, we wish to bound the last term by $S_i$, so it is enough to exhibit a $W$ such that $W: ||W||_{d*} \leq \lambda$ such that
\[
\sup_{Y\in K} \langle 2A-11^t-W, B_i-Y\rangle \leq S_i.
\]
For the sake of notation, let 
$D= 2A-11^t-W$.
The last inequality then becomes
\[
\langle D,B_i\rangle + \sup_{Y \in K}\langle D,-Y \rangle \leq S_i.
\]
which is equivalent to 
\[
 \sup_{Y \in K}\langle -D,Y \rangle \leq S_i-\langle D,B_i\rangle.
\]
We will now compute $$\sup_{Y \in K}\langle -D,Y \rangle$$.
$K$ is the hipercube, which gives the restrictions $0 \leq Y \leq 11^t$ and $Y=Y^t$. 
The lagragian is: 
\[
L = \langle D,-Y \rangle + \langle \Lambda,11^t-Y \rangle + \langle \eta,Y \rangle + \langle \Omega,Y-Y^t \rangle
= \langle D+\Lambda -\eta,-Y \rangle + \langle \Lambda,11^t\rangle + \langle \Omega, Y-Y^t\rangle\]
with $\Lambda,\eta \geq 0$.
It follows that
\[
\sup_{Y \in K}\langle -D,Y \rangle = \sup_{Y \in K} \inf_{\Lambda,\eta \geq 0} L = \begin{cases}
\langle 2A-11^t-W,-Y \rangle \ \text{if } Y \in K\\
- \ \infty \text{ in the other case.}
\end{cases}
\]
Exchanging infimum and supremum, we obtain  
\[
\sup_{Y \in K}\langle -D,Y \rangle = \inf_{\Lambda,\eta \geq 0}\sup_{Y \in K}L= \inf_{\Lambda,\eta \geq 0} 
\begin{cases}
\langle \Lambda, 11^t \rangle \ if \ D+\Lambda + \eta = 0 \ and \ \Omega =0.\rangle \\
\infty \text{ in the other case.}
\end{cases}
\]
Finally, this problem reduces to, 
\[
\inf_{\Lambda \geq0} \|\Lambda\|_1
\]
Subject to 
\[
\Lambda, \ D+\Lambda \geq 0.
\]
\end{proof}
\ddr{En realidad lo que hace lambda mayuscula es calcular la norma 1 de la matriz max(-D,0). Esta forma se puede obtener directamente sin la necesidad de dualidad ni del lagrangiano(ver notas viejas). Sin embargo escribo esta version pues es la que se us\'o en la programaci\'on. }
Note that the complexity of this problem depends on the complexity of computing $||W||_{d*}$
 More concretely,

\begin{cor} If $A$ has entries in $\{0,1\}$ then the optimal value of the following  programming problem equals $R_{\delta}(A)$.

\[
 \inf_{(\lambda,s_1,\dots, s_N,W,Z_1,..,Z_N,a)} \lambda\delta +\sum c_is_i + \sum_{i=1}^N c_i \mathds{1}^t Z_i \mathds{1}.
\]
Subject to
\begin{center}
\[
\begin{array}{l}
\lambda \geq 0\\
-Z_i \leq A-B_i  \leq Z_i \ for \ i=1,...,N\\
-a \leq 2A-11^t-W \leq a \\
\|W\|_{d*}\leq \lambda\\
a\leq s_i-\langle 2A-11^t-W, B_i\rangle\text{ for $i=1,\dots, N$}\\
\end{array}
\]
\end{center}


Where $\mathds{1}$ is a vector of ones of size $n$ and the inequalities in the first and second restriction are entry-wise. Moreover, the complexity of the problem is given by the complexity of $||\cdot||_{d*}$. In particular, if $\|\cdot\|_*$ is the norm $1$, the optimization problem is linear and if  $\|\cdot\|_*$ is the spectral norm, the optimization problem is semidefinite.  
\end{cor}
\begin{proof}
The optimization problem follows directly from theorem \ref{teoError}. The norm $1$ and norm $\infty$ are polyedricaly representable. The dual of the spectral norm, namely the nuclear norm is semidefinitely representable.
\end{proof}

%\mv{To do: probar este Teorema. Lo m\'as importante es escribirlo de manera muy expl\'icita para que se pueda implementar usando el hecho de que la norma nuclear es semidefinidamente representable}
%\ddr{Fresco, yo lo escribo para la proxima semana}

The problem of finding the best $A$ reduces to computing $\min_{A\in C} R_{\epsilon}(A)$ where $C$ is the set of all adjacency matrices of graphs (the set $C$ consists of $2^{\binom{n}{2}}$ matrices). 

This problem can be relaxed to the minimization problem over all $A\in K$ \mv{of which function? Note that the $1$-norm is representable by a linear function only for $A\in C$ and NOT in the rest of $K$}. Alternatively one can compute the risk for any choice $A\in K$ by recalling that for any matrix $D$ the norm $\|D\|_1$ equals the pointwise maximum of $2^{\binom{n}{2}}$ inequalities (because such inequalities are the extreme points of the unit ball in the dual $\ell_1$-norm)
\[\|D\|_1=\max \sum \epsilon_{ij}d_{ij}\]
and the maximum runs over all choices of $\epsilon_{ij}\in\{\pm 1\}$.
\mv{Can this be used to get a better relaxation, valid for all $A\in K$ and not only those $A$ with \{0,1\}-entries?}



\begin{theorem} 
The optimal value of the optimization problem
\[\beta:=\min_S \left(\lambda\epsilon +\sum_{i=1}^N c_is_i+\sum_{i=1}^Nc_i\mathds{1}^tZ_i\mathds{1}\right)\]
where $L$ is the set of $(\lambda, s_1,\dots, s_N, W,A)\in \RR\times\RR^N\times \RR^{n\times n}\times \RR^{n\times n}$ which satisfy the inequalities
\begin{center}
\[
\begin{array}{l}
\lambda \geq 0\\
-Z_i \leq A-B_i \leq Z_i \ for \ i=1,\dots, N \\
-a \leq 2A-11^t-W \leq a \\
\|W\|_{d*}\leq \lambda\\
a\leq s_i-\langle 2A-11^t-W, B_i\rangle\text{ for $i=1,\dots, N$}\\
0\leq A_{ij}\leq 1 \text{ for  $1\leq i,j\leq n$} \\
A = A^t
\end{array}
\]

\end{center}


\end{theorem}

\section{Unique recovery}


In this chapter we will introduce the problem of unique recovery giving contex and la little bit of structure to the problem we are working with.



Remember the fundametal problem we are dealing with: finding $A^*$: 

\[
A^*=\min_A \EE_B\left[||A-B||_1\right]
\]
Where we only know an iid sample  $B_1,..,B_N$ and $B_i ~ \mu$ with $\mu$ a measure over the matrices $n \times n$. In this section we will refer to $\mu$ as "the model of the $B_i$".

The stochastic bloc model, the Erdos Renyi model and the work of the previous section motivate the following definition:


\begin{definition}
A model (i.e a measure) $\mu$ over the set of $n \times n$ matrices  admits a $*-inherent$ structure if the minimization problem 

\[
A^*=\min_A \EE_B\left[||A-B||_*\right]
\]
has a \textbf{unique} minimizer $A^* \in \{0,1\}^{n\times n}$. In such case we call $A^*$ the *-inherent structure of the model $\mu$.

\end{definition}
We will focus only on 1-inherent structures. We will now define two random graph models that will help us motivate the previous defintion.

\begin{definition}
fix $n$. the Erdos-Renyi model $Erdos(n,p)$ is a graph with n vertices whose adjacency matrix is given by:
\[
D_{ij}=D_{ji}=\begin{cases}
1 \text{ with probability p and } i \neq j. \cr
0 \text{ with probability }1-p \text{ or if } i=j.
\end{cases}
\]
\end{definition}
In other words, vertices are contected with probability $p$ independent of other vertices.


Now we define the stochastic block model which is usually used to evaluate cluster recovery methods.


\begin{definition}
fix n. The stochastic block model is a random graph model given by the following parameters:

\begin{enumerate}
\item A number $n$ of vertices.
\item A partition $C_1,..,C_r$ of the vertices called communities.
\item A symmetric matrix $P$ of dimension $r\times r$ of edge probabilities.
\item Two vertices $u \in C_i$ and $v \in C_j$ are conected with probability $P_{ij}$.
\end{enumerate}
We will refer to this model as
$SB((C1,..,C_r),P)$
The "planted partition model", also sometimes refered as the stochastic block model is the special case when $P$ is constant $q$ in the Diagonal and constant $q$ outside of it. We will refer to this model as $PP((C1,..,C_r),p,q)$.
\end{definition}
Note that if $P_{ij}=p$ we recover the Erdos-Renyi model.

\begin{remark}
The Erdos-Renyi model $Erdos(n,\frac{1}{2})$ does not admit a 1-inherent structure as  both the matrices 
$A_1=11^t$ and $A_2=0$ satisfy:

\[
\min_A \EE_B\left[||A-B||_1\right]= \EE_B\left[||A_1-B||_1\right]=\EE_B\left[||A_2-B||_1\right]=\frac{n}{2}.
\]
where $B \sim Erdos(n,\frac{1}{2})$.
\end{remark}

\begin{remark}
The stocastic block model 
$SB((C1,..,C_r),P)$ admits a 1-inherent structure $A^*$ if $P$ satisfies the conditions:
\begin{enumerate}
\item $P_{ii}>\frac{1}{2} \ for \ i=1,..,r.$
\item $P_{ij} \leq \frac{1}{2} \  \forall  i \neq j$.
\end{enumerate}
Moreover, $A^*$ is given by a permutation of the rows of the matrix:

\[
D^* = 
\left[
\begin{array}{c c c c}
M_1 & 0 &\cdots & 0\\
0 & M_2 & \cdots & 0 \\ 
\vdots & \vdots & \ddots & \vdots \\
0 & 0& \cdots & M_r

\end{array}
\right] 
\]
Where  $M_i=11^t$ with dimension $c_i \times c_i$ and $c_i$ is the amount of vertices in $C_i$.
\end{remark}


\ddr{Esto debe ser f\'acil de probar usando subgradientes.}

\ddr{El siguiente teorema es el importante de esta secci\'on}.

\begin{theorem}[Unique recovery]
Let $\mu$ be a model with an 1-inherent structure $A^*$. let $B_1,..,B_N$ be an i.i.d sample with $B_i \sim \mu$.
As $\delta$ tends to $0$ and $N$ tends to infinity, the program 

\[
\hat{A}=\min_A\sup_{\nu\in B_{\delta}(\hat{\mathbb{P}_N})} \EE[\|A-B\|_1]
\]
recovers $A^*$, meaning that

\[
\hat{A}=A^*.
\]

We study the question of finding the best deterministic summary (i.e. adjacency matrix $A$ with entries in ${0,1}$) of a random graph $G$ from an i.i.d. sample $B_1,\dots, B_n$ of its adjacency matrix.

Ideally we would like $A$ to be the matrix for which the risk $\mathbb{E}[\|G-A\|_1]$ is minimized. The difficulty lies on the fact that, as in the usual machine learning setup, we know $G$ only through the information contained in the sample $B_1,\dots, B_n$ which is not sufficient to compute this risk. 

Instead, we minimize the upper bound $R_\epsilon(A):=\sup_{\mu\in D}\mathbb{E}[\|B-A\|_1]$ where $\mu$ ranges in the Wasserstein ball $D$ of radius $\epsilon$ centered at the empirical measure and $B \sim \mu$. We prove that for some choices of metric this problem leads to a tractable, consistent, convex optimization problem (LP, SOCP or SDP). We further discuss some concrete applications. In particular, the proposed approach allows us to recover the underlying cluster structure of $G$ if sufficiently many observations are given.

\end{theorem}



\newpage



\section{Recovering cluster structures in the stochastic block model}

Suppose $B$ is a random (undirected loopless) graph on $n$ vertices generated by the stochastic block model. This means that we fix a set partition $C_1,\dots, C_l$ of $[n]$ into sets we call clusters and real numbers $0\leq p_i,\bar{p}\leq 1$ for $i=1,\dots, l$. The edges of $B$ are independent random variables and an edge joins vertices $i,j$ with probability $p_t$ if $\{i,j\}\subseteq C_t$ for some cluster $C_t$ and with probability $\bar{p}$ if $\{i,j\}$ is not contained in any $C_t$. We let $O\subseteq [n]\times [n]$ be the set of pairs of vertices which are not simultaneously contained in any cluster. 

For an integer $N$ let $B_1,\dots, B_N$ be an independent sample of $N$ graphs with the distribution of $B$. Let $A^*$ be the $n\times n$ matrix with entries in $\{0,1\}$ which captures the underlying cluster structure, namely $A^*_{ij}=1$ iff there is a cluster $C_t$ which contains both $ij$. In this section we study the probability, as a function of $\delta$ that the optimization problem $\min_A\Delta(A)$
\[\Delta(A)= \delta\|2A-11^t\|_{*}+\frac{1}{N}\sum_{k=1}^N\|A-B_k\|_1\] 
has the correct cluster structure $A^*$ as a minimizer. For vertices $i,j\in [n]$ define $n_1(ij)$ (resp. $n_0(ij)$) the random variables which count the number of times that a given pair is (resp is not) an edge of some $B_j$, $j=1,\dots N$. Note that the $n_q(ij)$ for $q=0,1$ are binomial random variables.  

\begin{lemma} The following statements hold:
\label{lem: subdiff}
\begin{enumerate}
\item The subdifferential of $\frac{1}{N}\sum_{k=1}^N\|A-B_k\|_1$ at $A^*$ is the set of symmetric matrices $C$ satisfying the inequalities
\[ \frac{n_0(ij)-n_1(ij)}{N}\leq C_{ij}\leq 1 \text{, if $\{i,j\}\subseteq C_t$ for some $t$ and}\]
\[-1\leq C_{ij} \leq \frac{n_0(ij)-n_1(ij)}{N} \text{ if $\{i,j\}$ does not belong to any cluster. } \]
\item The subdifferential of $\delta\|2A-11^t\|_{*}$ at $A^*$ is given by the set of symmetric matrices of the form $2\delta C$ where $C$ has spectral norm $\|C\|\leq 1$ and satisfies $\langle C, 2A-11^t\rangle = n$.

\end{enumerate}

\end{lemma}
\begin{proof} $(1)$ Since the subdifferential is additive it suffices to understand the subdifferential of the absolute value. If $i,j\in C_t$ then $A^*_{ij}=1$ and the entry $ij$ of the subdifferential of the sum at $A^*$ is $[-1,1]$ for each $B_i$ containing the edge and it is $1$ for each $B_i$ for which $(ij)$ is not an edge. If $i,j$ is not contained in any cluster then $A^*_{ij}=0$ and the entry $ij$ of the subdifferential of the sum at $A^*$ is $-1$ for each $B_i$ which contains the edge $ij$ and $[-1,1]$ for each $B_i$ which does not, proving the claim. $(2)$ It is easy to prove that the subdifferential of any norm $\|\bullet\|$ at a point $X$ is given by those $C$ for which the dual norm $\|C\|_*\leq 1$ and $\langle C,X\rangle =\|X\|$. Claim $(2)$ follows because $\|2A-11^t\|=Tr(2A-11^t)=n$ where the first equality holds since $2A-11^t$ is positive semidefinite.\mv{Esto es obvio con solo dos clusters (la matriz es $uu^t$ donde $u$ es el vector con $1$'s en un cluster y $-1$'s en el complemento pero hay que demostrarlo para tres o mas)}.
\end{proof}


\begin{lemma} If $\delta=0$ then 
\[\PP\{A^*\in\argmin\Delta \} =\prod_{ij\in O} \PP\{n_1(ij)\leq n_0(ij)\} \prod_{t=1}^k \left(\prod_{(ij)\in C_t}\PP\{n_0(ij)\leq n_1(ij)\}\right)\]
where $\PP\{n_0(ij)\leq n_1(ij)\}$ is given by the following formula 
\mv{Ejercicio para Daniel: Encontrar una f\'ormula, es la probabilidad de que haya mas caras que sellos en $N$ lanzamientos de una moneda trucada donde las probabilidades de la moneda dependen s\'olo de la arista $ij$}.
\end{lemma}
\begin{proof} The matrix $A^*$ is a minimizer of the above convex function if and only if its subdifferential at $A^*$ contains the matrix $0$. By part $(1)$ of the previous Lemma this occurs if and only if $\frac{n_0(ij)-n_1(ij)}{N}\leq 0$ for $(ij)$ in a cluster and 
$\frac{n_0(ij)-n_1(ij)}{N}\geq 0$ for $(ij)$ in $O$. Independence of the edges then implies the above formula.  
\end{proof}
\begin{remark} Could the set of minimizers be larger? If $A'$ has at least one entry $A_{ij}\in (0,1)$ the corresponding component in the subgradient is the constant $n_0(ij)-n_1(ij)$ and this equals zero with much smaller probability, precisely when both terms equal to $\frac{N}{2}$. In particular it is impossible if $N$ is odd and in this case $A^*$ is the only minimizer.
\end{remark}

Next we ask whether it is possible to increase the probability of correct recovery by allowing $\delta>0$. By Lemma~\ref{lem: subdiff} $A^*\in \argmin \Delta$ if and only if there exists $S$ in the subdifferential of $\delta\|2A-11^t\|_*$ at $A=A^*$ such that $-S$ belongs to the subdifferential of $\frac{1}{N}\sum_{k=1}^N\|A-B_k\|_1$ at $A^*$. 

The most immediate way to do this would be to find an $S$ with $S_{ij}\leq 0$ negative on edges $ij\in C_t$ and $S_{ij}\geq 0$ on edges of $O$. More precisely we would like to find a best such $C$ by solving the optimization problem:

\[
\min \left(\sum_{t}\sum_{(ij)\in C_t} C_{ij}\right)-\sum_{(ij)\in O} C_{ij} \text{ s.t. $\|B\|\leq 1$, $\langle C, 2A^*-11^t\rangle = n$} 
\]

However it is easy to see that we cannot do this improvement simultaneously in all components, because $n=\langle S, 2A-11^t\rangle = Tr(S)+\sum_{ij \in O^c} S_{ij} -\sum_{ij \in O} S_{ij}\leq n+u$ where $u$ is the objective function in the problem above. We conclude that $u$ must be nonnegative so we cannot improve simultaneously in all directions at once.
In the following section we discuss how tradeoffs between components explain the improved recovery probability induced by the spectral norm.

\section{Estimating recovery probabilities}

Let $\Gamma$ be the symmetric matrix with zero diagonal and off-diagonal entries given by
$\Gamma_{ij}=\frac{n_0(ij)-n_1(ij)}{N}$. By Lemma~\ref{lem: subdiff} a symmetric matrix $C$ lies in the subdifferential if and only if it satisfies the inequalities
\[ \Gamma_{ij}\leq C_{ij}\leq 1 \text{, if $\{i,j\}\subseteq C_t$ for some $t$ and}\]
\[-1\leq C_{ij} \leq \Gamma_{ij} \text{ if $\{i,j\}$ does not belong to any cluster. } \]
To simplify these inequalities we define a linear operator $\widetilde{\bullet}$ on symmetric matrices by the formula
\[ \widetilde{A} = 
\begin{cases}
A_{ij}\text{ if $i=j$ or $ij\in I$ and}\\
-A_{ij}\text{ if $ij\in O$.}
\end{cases}
\]
in this language $C_{ij}$ belongs to the subdifferential if and only if $\widetilde{\Gamma}_{ij}\leq \widetilde{C}_{ij}\leq 1$ for $i\neq j$.
The following key result gives sufficient conditions for the true cluster structure $A^*$ to be a minimizer of the proposed optimization problem. In order to describe it we introduce the following notation. 

\begin{definition} Let $\delta$ be a positive real number. For a symmetric matrix $\Gamma$ define the quantities
\[b(\Gamma,\delta):=\sum_{i\neq j} \max\left(\widetilde{\Gamma}+\frac{2\delta}{n}11^t,0\right)
\text{ and } a(\Gamma,\delta):=\sum_{i\neq j} \max\left(-\widetilde{\Gamma}-\frac{2\delta}{n}11^t,0\right)
\]\end{definition}
The quantity $b(\Gamma,\delta)$ (resp. $a(\Gamma,\delta)$) measures the total amount by which the matrix $\widetilde{-\frac{2\delta}{n}11^t}$ fails (resp. succeeds) to be in the subdifferential of Lemma~\ref{lem: subdiff} in the sense that it sums over all $ij$ the amount by which the inequalities $\widetilde{\Gamma_{ij}}\leq \frac{2\delta}{n} 11^t$ fail (resp. succeed). The key point of the following Theorem is that if the inequality fails by less than it succeeds then the subdifferential of the spectral norm is sufficiently rich so as to allow us to redistribute these quantities. In this sense the following Theorem explains the success of the spectral norm in cluster recovery algorithms. 


\begin{theorem} Assume there are only two clusters. If $b(\Gamma,\delta)\leq \min(\delta, a(\Gamma,\delta))$ then $A^*$ is a minimizer of the optimization problem $\min_A\Delta(A)$. 
\end{theorem}
\begin{proof} We will show that there exists a matrix $C_{ij}$ such that $-C_{ij}\in \partial \left(\delta\|2A-11^t\|\right)(A^*)$ for which $\widetilde{\Gamma_{ij}}\leq \widetilde{C_{ij}}$ for $i\neq j$. It will then follow that $0=C-C$ belongs to the subdifferential of $\Delta(A)$ at $A^*$ and thus $A^*$ is a minimizer as claimed.

Recall that $-C\in \partial \left(\delta\|2A-11^t\|\right)(A^*)$ if and only if it satifies the conditions
\[ \langle H^* , C\rangle =-2\delta n\text{ and }\|C\|\leq 2\delta \]
where $H^*:=2A^*-11^t$ and $\|\bullet\|$ is the spectral norm. 
Both of these conditions are satisfied by setting $C=-\frac{2\delta}{n} H^*$. However this choice of $C$ will not, in general, satisfy the inequalities $\widetilde{\Gamma}_{ij}\leq \widetilde{-\frac{2\delta}{n} H^*}_{ij}=-\frac{2\delta}{n}11^t_{ij}$ for $i\neq j$. 

We will adjust our candidate for $\widetilde{C}$ by adding to it a transportation matrix $\widetilde{K}$ that will guarantee that all these inequalities are satisfied when $b(\Gamma,\delta)\leq a(\Gamma, \delta)$. Crucially we will choose $\widetilde{K}$ so that $K$ satisfies $KH^*=H^*K=0$ allowing us to control the spectral norm of $C$.

Since $b(\Gamma,\delta)\leq a(\Gamma,\delta)$ there exists a way to redistribute the quantity $b(\Gamma,\delta)$ by substracting it from the $ij$ for which $-\frac{\delta}{n}\leq \widetilde{\Gamma}_{ij}$  and adding it into those $st$ for which $\widetilde{\Gamma}_{st}\leq -\frac{\delta}{n}$. More specifically, if $b=\widetilde{\Gamma_{ij}}+\frac{2\delta}{n}>0$ then there exists a set $(i_1,j_1),\dots, (i_t,j_t)$ of non-diagonal entries and nonnegative constants $\gamma_{i_s,j_s}$ summing to one 
such that $\widetilde{\Gamma}_{i_sj_s}+\frac{2\delta}{n}+ b\gamma_{i_s,j_s}\leq 0$. Define the off-diagonal elements of $\widetilde{C}$ by
\[\widetilde{C}:=-\frac{2\delta}{n} 11^t + \sum_{i_s,j_s} b\gamma_{i_s,j_s}(-e_{ij}+e_{i_s,j_s})\]
where $e_{ij}$ is the symmetric matrix with one in positions $i,j$ and $j,i$ and zeroes otherwise. More generally, let $B$ be the set of paris $(i,j)$ with $i<j$ such that $\widetilde{\Gamma_{ij}}+\frac{2\delta}{n}>0$. If $b(\Gamma,\delta)\leq a(\Gamma,\delta)$ then for each $ij\in B$ there exist nonnegative constants $\gamma^{(ij)}_{st}$ such that $\sum_{s < t} \gamma^{(ij)}_{st}=1$ and for which the matrix
\[\widetilde{C}:=-\frac{2\delta}{n}11^t+\sum_{ij\in B} \sum_{s<t} \left(\widetilde{\Gamma_{ij}}+\frac{2\delta}{n}\right)\gamma_{st}^{(ij)} (-e_{ij}+e_{st})\]  
satisfies $\widetilde{\Gamma}_{ab}\leq \widetilde{C}_{ab}$ for all $a\neq b$. We will show that if $b(\Gamma,\delta)\leq 2\delta$ then the matrix $C$ with off-diagonal entries given by
\[C_{ab}=-\frac{2\delta}{n}H^*_{ab} + \sum_{ij\in B} \sum_{s<t} \left(\widetilde{\Gamma_{ij}}+\frac{2\delta}{n}\right)\gamma_{st}^{(ij)} \widetilde{(-e_{ij}+e_{st})_{ab}}\]
lies in $-\partial \left(\delta\|2A-11^t\|_*\right)(A^*)$ for some choice of diagonal, proving the Theorem. 
For a pair of indices $i<j$ let $\epsilon_{ij}=1$ if $ij\in I$ and $\epsilon_{ij}=-1$ if $ij\in O$. Define the matrix $t_{ij}$ by $t_{ij}=\epsilon_{ij} e_{ij}-e_{ii}-e_{jj}$ and note that $t_{ij}$ satisfies the following three properties: 
\begin{enumerate}
\item The equalities $H^*t_{ij}=0=t_{ij}H^*$ hold. If $ij\in O$ this happens only when there are exactly two clusters and this is the only point in the proof where this assumption is used.
\item The off-diagonal entries of $\widetilde{t_{ij}}$ are equal to those of $e_{ij}$. In particular the off-diagonal entries of $\widetilde{(-e_{ij}+e_{st})_{ab}}$ are always equal to those of $-t_{ij}+t_{st}$.
\item The inequality $\|-t_{ij}+t_{st}\|\leq 2$ holds for all $ij$ and $st$. This is immediate via direct calculation (the inequality is strict only if $|\{i,j\}\cap\{s,t\}|\geq 1$ and in this case it can take values of $\sqrt{3}$ and $0$).  
\end{enumerate}
If $C$ denotes the matrix given by
\[C:= -\frac{2\delta}{n}H^* + \sum_{ij\in B} \sum_{s<t} \left(\widetilde{\Gamma_{ij}}+\frac{2\delta}{n}\right)\gamma_{st}^{(ij)} (-t_{ij}+t_{st})\]
then the following properties hold:
\begin{enumerate}
\item The off-diagonal entries agree with those in our previous expression so $\widetilde{\Gamma}_{ab}\leq \widetilde{C}_{ab}$ for all $a\neq b$ and thus $C$ is in the subdifferential of Lemma~\ref{lem: subdiff}.
\item Since $H^*t_{ij}=0=t_{ij}H^*$ the equality $\langle H^*, C\rangle =\langle H^*,-\frac{2\delta}{n}H^*\rangle = -2\delta n$ holds and moreover
\[ \| C\|=\max\left(\left\|-\frac{2\delta}{n}H^*\right\|, \left\|\sum_{ij\in B} \sum_{s<t} \left(\widetilde{\Gamma_{ij}}+\frac{2\delta}{n}\right)\gamma_{st}^{(ij)} (-t_{ij}+t_{st})\right\|\right).\] 
\end{enumerate}
The operator norm of the first term in the maximum equals $2\delta$ and that of the second term is bounded by $2b(\Gamma,\delta)$ by the triangle inequality and the definition of $b(\Gamma,\delta)$. We conclude that $\|C\|$ is bounded by $2\delta$ because $b(\Gamma,\delta)\leq \delta$. As a result $-C\in \partial\left(\|2A-11^t\|\right)(A^*)$ proving the Theorem.
\end{proof}

\mv{Como extendemos este razonamiento al caso de tres o m\'as clusters? Debe ser posible utilizar otras matrices de transporte para este caso que vivan en el kernel. Algo interesante es que con tres clusters es necesario que los promedios de una matriz aniquilada por $H^*$ dentro de cada uno de los bloques $C_i\times C_j$ deban ser cero.} 

Using the previous Theorem we now estimate the probabilities of perfect recovery of the correct cluster structure.

\begin{cor} Given $\delta>0$ the probability that $A^*$ is a minimizer of the optimization problem $\min_A\Delta_{\delta}(A)$ is bounded below by $\mathbb{P}\{ b(\Gamma,\delta)\leq \min \left(a(\Gamma,\delta), 2\delta\right)\}$.  
This quantity can be computed in terms of the joint distribution of two sums over all edges of independent random variables, because the following idetities hold:
\[b(\Gamma,\delta)+a(\Gamma,\delta)=\sum_{ij} \left|\widetilde{\Gamma}_{ij}+\frac{2\delta}{n}\right|\]
\[b(\Gamma,\delta)-a(\Gamma,\delta)=\sum_{ij} \left(\widetilde{\Gamma}_{ij}+\frac{2\delta}{n}\right)\]
and in particular, 
Assuming the following inequalities hold:
\begin{itemize}
\item  $- 2N\delta(n-1)-N \left [\sum_{C_k} \sum_{ij \in C_k}(q_k-p_k)+ \sum_{ij \in O}(\bar{q}-\bar{p}) \right ] \geq 0$
\item $2\delta + \sum_{i\neq j}\eta_{ij} \geq 0$
\end{itemize}
Where

\[
\eta_{ij}= {}\begin{cases}
\sum_{s=1}^d \binom{N}{s}p_k^sq_k^{N-s}\left [(q_k-p_k)+\frac{2\delta}{n}\right ] \text{ if } (i,j) \in C_k \text{ for some k}. \cr
\sum_{s=1}^d \binom{N}{s}\bar{q}^s\bar{p}^{N-s}\left [(\bar{p}-\bar{q})+\frac{2\delta}{n}\right ] \text{ if  }(i,j)\in O.
\end{cases}
\]
and  $d =  \lfloor\frac{N(2\delta+n)}{2n}\rfloor.$

Then,
\[\mathbb{P}\{ b(\Gamma,\delta)\leq \min \left(a(\Gamma,\delta), \delta\right)\}\geq
1- exp \left \{-2 \frac{\left (2\delta + \sum_{i\neq j}\eta_{ij}\right )^2}{\sum_{i\neq j}(2+\frac{4\delta}{n})^2}     \right \} - 
exp\left\{-N \left ( \frac{n-1}{n} \right ) \left (\delta +\frac{M}{n-1}\right )^2 \right\}
\]  
%\mv{TAREA: Encontrar la mejor f\'ormula posible. Esto debe poder hacerse usando desiguladades de concentraci\'on junto con las igualdades del Corolario. Notar que esta desigualdad es independiente de la prueba del Teorema anterior y es solo un problema de probabilidad multinomial.}
\end{cor}
\begin{proof}

Let $\delta>0$.By the previous theorem, we know that 
$b(\Gamma,\delta)\leq \min \left(a(\Gamma,\delta), 2\delta\right)\}$ is sufficient for $A*$ to belong to the subdifferential, so this assures that
 the probability that $A^*$ is a minimizer of the optimization problem $\min_A\Delta_{\delta}(A)$ is bounded below by $\mathbb{P}\{ b(\Gamma,\delta)\leq \min \left(a(\Gamma,\delta), 2\delta\right)\}$.

Notice the following fact:
\[
\begin{aligned}
\PP\{b(\Gamma,\delta) \leq \min(a(\Gamma,\delta),2\delta \} = {} & 1-\PP\{b(\Gamma,\delta) > a(\Gamma,\delta) \ or \  b(\Gamma,\delta)>2\delta \} \\
& \geq 1-(\PP\{b(\Gamma,\delta)>a(\Gamma,\delta)\}+ \PP\{b(\Gamma,\delta)> 2\delta\})
\end{aligned}
\]
We will thus compute independently $\PP\{b(\Gamma,\delta)>a(\Gamma,\delta)\}$ and $ \PP\{b(\Gamma,\delta)> 2\delta\})$.
The following facts will be useful in the proof. Please keep them in mind:
\begin{enumerate}
\item $C_k$ denotes the $k$-th cluster, for $k \in \  \{1,..,l\}$.
\item the set $I\subseteq [n]\times [n]$ denotes the set of $\{i,j\}$ such the $A^*_{ij}=1$.
\item $O = [n]\times[n]\setminus I$.
\item for $(i,j)\in \ C_k$, $\EE\{\Gamma_{ij}\}= q_k-p_k. $
\item for $(i,j)\in O, \EE\{\Gamma_{ij} \}= \bar{q}-\bar{p}$
\item $\Gamma_{ij}=\frac{n_0(ij)-n_1(ij)}{N}=\frac{N-2n_1(ij)}{N}= \frac{2n_0(ij)-N}{N}$.
\item $-N \leq N\Gamma_{ij}\leq N$ for all $(i,j)$.
\end{enumerate}

We begin by bounding by the probability $\PP\{b(\Gamma,\delta)>a(\Gamma,\delta)\}$.

By the definition of $a$ and $b$ it is clear that
\[b(\Gamma,\delta)-a(\Gamma,\delta)=\sum_{i\neq j} \left(\widetilde{\Gamma}_{ij}+\frac{2\delta}{n}\right) = \sum_{i\neq j} \widetilde{\Gamma}_{ij} + \frac{2\delta n(n-1)}{n}. \]

Now, 

\[
\begin{aligned}
 \sum_{i\neq j}\widetilde{\Gamma}_{ij} + 2\delta (n-1) \geq 0 \Leftrightarrow  \sum_{ij \in I}\widetilde{\Gamma}_{ij} + \sum_{ij \in O}\widetilde{\Gamma}_{ij}  + 2\delta(n-1) \geq 0  \\
 \Leftrightarrow \sum_{ij \in I}\Gamma_{ij} - \sum_{ij \in O} \Gamma_{ij} + 2\delta(n-1) \geq 0 
\end{aligned}
\]
By the definition of $\widetilde{\bullet}$.
Splitting the sum over $I$ on the sets $C_k, \ k \in \{1,..,l\}$ and substracting the mean of the $\Gamma_{ij}$ it follows that


\[
 \sum_{i\neq j} \widetilde{\Gamma}_{ij} + 2\delta (n-1) \geq 0 
\]
\[
\Leftrightarrow \sum_{C_k} \sum_{ij \in C_k} \left(\Gamma_{ij}-(q_k-p_k)\right) - \sum_{ij \in O}\left(\Gamma_{ij}-(\bar{q}-\bar{p})\right)  + 2\delta(n-1)+M \geq 0  
\]

Where $M$ is defined as:
\[
M:= \sum_{C_k} \sum_{ij \in C_k}(q_k-p_k)+ \sum_{ij \in O}(\bar{q}-\bar{p}).
\]

The quantity $M$ is very important and we will discuss about it shortly after.
Finally, we multiply by $N$ the latter expression to show the role of $N$ in the bound to be given next: 

\[
 \sum_{i\neq j} \widetilde{\Gamma}_{ij} + 2\delta (n-1) \geq 0 
\]
\[
\Leftrightarrow \sum_{C_k} \sum_{ij \in C_k} \left(N\Gamma_{ij}-N(q_k-p_k)\right) - \sum_{ij \in O}\left(N\Gamma_{ij}-N(\bar{q}-\bar{p})\right)\geq  - 2N\delta(n-1)-NM.  
\]

The previous expression is a sum of independent random variables, bounded by $N$ with mean $0$, so we can apply Hoeffding's inequality (some version of...) to obtain the bound
\[
\PP\{ \sum_{i\neq j} \widetilde{\Gamma}_{ij}+ 2\delta (n-1) \geq 0 \} \leq exp\left\{-\frac{2[2(n-1)\delta N+NM]^2}{2Nn(n-1)}\right\}=exp\left\{-N \left ( \frac{n-1}{n} \right ) \left (\delta +\frac{M}{n-1}\right )^2 \right\}.
\]
Note that we assumed that the quantity $- 2N\delta(n-1)-NM$ is positive in order to use Hoefdding's inequality. Hence the asumption in the lemma. 


We continue by bounding $\PP\{b(\Gamma,\delta)> 2\delta\})$.

Recall that 

\[b(\Gamma,\delta):=\sum_{i\neq j} \max\left(\widetilde{\Gamma_{}}_{ij}+\frac{2\delta}{n},0\right)
\]
For $i\neq j$ define the random variable 
\[
Y_{ij}:= w_{ij}\left(\widetilde{\Gamma_{}}_{ij}+\frac{2\delta}{n}\right).
\]
Where $w_{ij}=1$ if $\widetilde{\Gamma}_{ij}+\frac{2\delta}{n} \geq 0$ and $0$ otherwhise. It is clear that $b(\Gamma,\delta) = \sum_{i\neq j}Y_{ij}$.

Now, note that for all $(i,j)$ the random variable $Y_{ij}$ is bounded $1+\frac{2\delta}{n}$ i.e 
$|Y_{ij}|\leq 1+\frac{2\delta}{n}$.
We proceed to compute the expected value of $Y_{ij}$.
Fix $(i,j) \in C_k$ for some $k$.
By the law of total expectation we have that
\[
\begin{aligned}
\EE\{Y_{ij}\}=\EE \left \{w_{ij}\left ( \Gamma_{ij} +\frac{2\delta}{n}\right ) \right\}= \EE\left \{ w_{ij}\left ( \Gamma_{ij} +\frac{2\delta}{n}\right ) | w_{ij} = 1\right \}\PP\{w_{ij}=1\}& \\
+ \EE\left \{ w_{ij}\left ( \Gamma_{ij} +\frac{2\delta}{n}\right ) | w_{ij} = 0\right \}\PP\{w_{ij}=0\} & \\
=\EE\left \{ \left ( \Gamma_{ij} +\frac{2\delta}{n}\right )\right \}\PP\{w_{ij}=1\}
\end{aligned}
\]

Then, $\EE\{Y_{ij}\} =\PP\{w_{ij}=1\}[(q_k-p_k)+\frac{2\delta}{n}]$.

\[
w_{ij} =1 \Leftrightarrow \Gamma_{ij} + \frac{2\delta}{n} \geq 0  \Leftrightarrow \frac{N-2n_1(ij)}{N} \geq -\frac{2\delta}{n} \Leftrightarrow n_1(ij) \leq \frac{N(2\delta+n)}{2n}.
\]
For $(i,j) \in C_k$, $n_1(ij)\sim Bin(N,p_k) $ and using the distribution of the Binomial we have that

\[
\PP\{w_{ij}=1\} = \sum_{s=1}^d \binom{N}{s}p_k^sq_k^{N-s} \text{   Where } d =  \lfloor\frac{N(2\delta+n)}{2n}\rfloor. 
\]




For $(i,j) \in O$ the computations are similar and by recalling that $\widetilde{\Gamma}_{ij} = -\Gamma_{ij}$,
\[
\EE\{Y_{ij}\} =\PP\{w_{ij}=1\} \left [(\bar{p}-\bar{q})+\frac{2\delta}{n}\right ].
\]

Moreover, 
\[
w_{ij}=1 \Leftrightarrow \frac{2\delta}{n} \geq \Gamma_{ij} \Leftrightarrow n_0(ij) \leq \frac{N(2\delta+n)}{2n}
\]

For $(i,j) \in O$, $n_0(ij)\sim Bin(N,\bar{q}) $ and using the distribution of the Binomial we have that

\[
\PP\{w_{ij}=1\} = \sum_{s=1}^d \binom{N}{s}\bar{q}^s\bar{p}^{N-s} \text{   Where } d =  \lfloor\frac{N(2\delta+n)}{2n}\rfloor. 
\]

To simplify the notation, define $\eta_{ij}$ as follows:

\[
\eta_{ij}=\EE\{Y_{ij}\}= {}\begin{cases}
\sum_{s=1}^d \binom{N}{s}p_k^sq_k^{N-s}\left [(q_k-p_k)+\frac{2\delta}{n}\right ] \text{ if } (i,j) \in C_k \text{ for some k}. \cr
\sum_{s=1}^d \binom{N}{s}\bar{q}^s\bar{p}^{N-s}\left [(\bar{p}-\bar{q})+\frac{2\delta}{n}\right ] \text{ if  }(i,j)\in O.
\end{cases}
\]
Where $d =  \lfloor\frac{N(2\delta+n)}{2n}\rfloor.$

The random variables $Y_{ij}$ are independent, bounded and $2\delta + \sum_{i\neq j}\eta_{ij}$ is positive by assumption. Therefore, by Hoeffding's inequality, we obtain the following result:

\[
\PP\{b(\Gamma,\delta)\geq 2\delta\} = \PP \left \{\sum_{i \neq j}Y_{ij}-\sum_{i \neq j}\eta_{ij} \geq 2\delta + \sum_{i\neq j}\eta_{ij}\right \} \leq exp \left \{-2 \frac{\left (2\delta + \sum_{i\neq j}\eta_{ij}\right )^2}{\sum_{i\neq j}(2+\frac{4\delta}{n})^2}     \right \} 
\]


Finally, we conclude that

\[
\PP\{b(\Gamma,\delta) \leq \min(a(\Gamma,\delta),2\delta \} \geq 1- exp \left \{-2 \frac{\left (2\delta + \sum_{i\neq j}\eta_{ij}\right )^2}{\sum_{i\neq j}(2+\frac{4\delta}{n})^2}     \right \} - 
exp\left\{-N \left ( \frac{n-1}{n} \right ) \left (\delta +\frac{M}{n-1}\right )^2 \right\}
\]
\end{proof}


\ddr{Varias cosas que decir: la cantidad $\frac{\left (2\delta + \sum_{i\neq j}\eta_{ij}\right )^2}{\sum_{i\neq j}(2+\frac{4\delta}{n})^2} $
parace dificil de entender, asi que hice una pequena funcion para calcular su valor. Para q = 0.3, p = 0.8, dos clusters de tamano 5, n=10 N=3 y delta = 1.5 este valor da 161. Por lo que proba de $b \geq 2\delta \geq exp(-161) \sim 10^{-140}$. Para $N=1$, da $81$ y $b \geq 2\delta \geq exp(-81) \sim 10^{-36}$ Considero que el siguiente lema deberia tratar de entender esta cantidad (al igual que la cantidad M), en particular dar algun tipo de condiciones suficientes para que se satisfagan las condiciones del lema. Por otro lado, ahi dos generalizaciones interesantes que podemos hacer:
\begin{itemize}
\item que pasa si la condicion de intependencia por una condicion de martingala y usar la desigualdad de Azuma (que generaliza Hoeffding). Probablemente podamos extender resultados a modelos mas generales del stocastic block model.
\item Que pasa si cambiamos la distribucion de $\Gamma$ (que sabemos es binomial) por algo mas general? podemos usar esto para trabajar con grafos pesados? se pueden generar grafos usando este conocimiento sobre $\Gamma$?
\end{itemize}
}

\end{document}
Note that $\Gamma$ is the set of variable endpoints in the inequalities of Lemma~\ref{lem: subdiff}. Accor






Our following Lemma gives us a decomposition Theorem for symmetric matrices. This decomposition will be very useful when computing spectral norms in the two-block case.
Suppose that we have only two blocks $C_1$ and $C_2$ and let $u$ be the vector given by
\[u_i=\begin{cases}
1\text{ if $i\in C_1$}
-1\text{ if $i\in C_2$}
\end{cases}
\]
Define $H^*:=uu^t$ and for $i=1,\dots, n$ let $e_{ii}$ be the $n\times n$ matrix whose only nonzero entry is in position $ii$ and has value $1$ 


\begin{lemma} Assume $|C_1|\neq |C_2|$ If $A$ is a symmetric matrix then there exist unique constants $\lambda_1,\dots, \lambda_{n-1}$ and a matrix $K$ such that $KH^*=H^*K=0$ such that
\[A= \frac{\langle H^*,A\rangle }{n^2} H^* +\sum_{i=1}^{n-1}\lambda_1(e_{i,i}-e_{i+1,i+1})+K.\]
And in particular the following equality holds  
\[ \|A\| =\max(A-K,K).\]
\end{lemma}
\begin{proof} The $n$ vectors in $v^tH^*$ and $v^t(e_{i,i}-e_{i+1,i+1})$ for $i=1,\dots, n-1$ are linearly independent in $\RR^n$ because the sum of the entries of all but the first one vanish. It follows that $v^tA$ can be expressed uniquely as a linear combination of them 


\end{proof}










\end{document}