{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "using JuMP \n",
    "using Mosek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Script para el aprendizaje de grafos usando la metrica de Wasserstein, pero esta vez con la funcion de \n",
    "#costo ||.||1 que vuelve al problema lineal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# @param:weights : weights of the observed matrices. Sum of weights must be 1. and non-negative. the size is equal to the amount\n",
    "#of observed graphs.\n",
    "#@param: delta : size of the ball of the Wasserstein metric. delta > 0.\n",
    "#@param: A : cluster form of the data. A entries must be 0 or 1. \n",
    "#@param: B : vector of matrices containing the observations of the graph B. If X,Y are matrices, an array of matrices\n",
    "#is of the form Array[X,Y].\n",
    "#@param: n : the dimension of the matrices.\n",
    "#@param:N : number of observations.\n",
    "#The set over which the is problem is optimized is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "learnLinearGraph (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function learnLinearGraph(weights,delta,B,n)\n",
    "    N = length(B)\n",
    "    one =ones(n)\n",
    "    m = Model(solver=MosekSolver())\n",
    "    @variable(m,lambda>=0)\n",
    "    @variable(m,S[1:N])\n",
    "    @variable(m,A[1:n,1:n], Symmetric)\n",
    "    #W debe ser simetrica? Creo que no.\n",
    "    @variable(m, W[1:n,1:n],Symmetric)\n",
    "    #Q es una variable auxiliar, juega el papel de Lambda en la demostracion\n",
    "    @variable(m,Q[1:n,1:n],Symmetric)\n",
    "    # u es una variable auxiliar para calcular la norma infinito de W.\n",
    "    @variable(m,u>=0)\n",
    "    #Z es un conjunto de matrices que permite calcular la norma 1 de A-B[i].\n",
    "    Z = [@variable(m, [1:n, 1:n], Symmetric) for p in 1:N]\n",
    " \n",
    "    #Constrains...\n",
    "    #constrains for lambda already in the definition of the varible.\n",
    "    #Constrains for A.\n",
    "    @constraint(m,A.>=0)\n",
    "    @constraint(m,A.<=1)\n",
    "    #Constrains for Z.\n",
    "    [@constraint(m, -Z[p].<= A-B[p]) for p in 1:N]\n",
    "    [@constraint(m,  A-B[p].<=Z[p]) for p in 1:N]\n",
    "    #Constrains for W. \n",
    "    @constraint(m,u<=lambda)\n",
    "    @constraint(m,u .>=W)\n",
    "    @constraint(m,u .>=-W)\n",
    "     #Constrains for Q.\n",
    "    @constraint(m,Q .>=-(2*A-one*one'-W))\n",
    "    @constraint(m,Q .>=0)\n",
    "    [@constraint(m, ones(n)'*Q*ones(n)<=S[p]-trace((2*A-one*one'-W)'*B[p])) for p in 1:N]\n",
    "    #Objetivo\n",
    "    @objective(m,Min,lambda*delta+ dot(weights,S+prod.(Z,n)))\n",
    "    #TT = STDOUT # save original STDOUT stream\n",
    "    #redirect_stdout()\n",
    "    solve(m)\n",
    "    #redirect_stdout(TT) # restore STDOUT\n",
    "    return([getvalue(A),getvalue(S),getvalue(lambda),getvalue(W),getobjectivevalue(m)])\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "learnLinearError (generic function with 1 method)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function learnLinearError(weights,delta,A,B,n)\n",
    "    N = length(B)\n",
    "    one =ones(n)\n",
    "    m = Model(solver=MosekSolver())\n",
    "    @variable(m,lambda>=0)\n",
    "    @variable(m,S[1:N])\n",
    "    #W debe ser simetrica? Creo que no.\n",
    "    @variable(m, W[1:n,1:n],Symmetric)\n",
    "    #Q es una variable auxiliar, juega el papel de Lambda en la demostracion\n",
    "    @variable(m,Q[1:n,1:n])\n",
    "    # u es una variable auxiliar para calcular la norma infinito de W.\n",
    "    @variable(m,u>=0)\n",
    "    #Z es un conjunto de matrices que permite calcular la norma 1 de A-B[i].\n",
    "    Z = [@variable(m, [1:n, 1:n], Symmetric) for p in 1:N]\n",
    "    #Constrains...\n",
    "    #constrains for lambda already in the definition of the varible.\n",
    "    #Constrains for Z.\n",
    "    [@constraint(m, -Z[p].<= A-B[p]) for p in 1:N]\n",
    "    [@constraint(m,  A-B[p].<=Z[p]) for p in 1:N]\n",
    "    #Constrains for W. \n",
    "    @constraint(m,u<=lambda)\n",
    "    @constraint(m,u .>=W)\n",
    "    @constraint(m,u .>=-W)\n",
    "    #Constrains for Q.\n",
    "    @constraint(m,Q .>=-(2*A-one*one'-W))\n",
    "    @constraint(m,Q .>=0)\n",
    "    [@constraint(m, ones(n)'*Q*ones(n)<=S[p]-trace((2*A-one*one'-W)'*B[p])) for p in 1:N]\n",
    "    #Objetivo\n",
    "    @objective(m,Min,lambda*delta+ dot(weights,S+prod.(Z,n)))\n",
    "    #TT = STDOUT # save original STDOUT stream\n",
    "    #redirect_stdout()\n",
    "    solve(m)\n",
    "    #redirect_stdout(TT) # restore STDOUT\n",
    "    return([getvalue(Q),getvalue(lambda),getvalue(S),getobjectivevalue(m)])\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 0.6.0",
   "language": "julia",
   "name": "julia-0.6"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "0.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
